# ุฎุทุฉ ูุธุงู ุงูุฃุฑุดูุฉ ุงูุดุงูู - ุงูุฌุฒุก ุงูุซุงูู: ุงูุชุทุจูู ุงูุชููู

## ๐ ูุธุฑุฉ ุนุงูุฉ
ูุฐุง ุงูุฌุฒุก ุงูุซุงูู ูู ุฎุทุฉ ุชุทููุฑ ูุธุงู ุงูุฃุฑุดูุฉ ุงูุดุงููุ ูุฑูุฒ ุนูู ุงูุชุทุจูู ุงูุชููู ุงููุนูู ูููุธุงูุ ุจูุง ูู ุฐูู ุฅูุดุงุก ูุงุนุฏุฉ ุงูุจูุงูุงุช ูุงูููุฏ ููุงุฌูุงุช ุงููุณุชุฎุฏู.

---

## ๐๏ธ ุงููุฑุญูุฉ ุงูุซุงููุฉ: ุฅูุดุงุก ุงูุจููุฉ ุงูุชุญุชูุฉ

### ๐๏ธ ุฅูุดุงุก ูุงุนุฏุฉ ุจูุงูุงุช ุงูุฃุฑุดูู

#### ุงูุฎุทูุฉ 1: ุฅูุดุงุก ูุงุนุฏุฉ ุงูุจูุงูุงุช
```sql
-- ุฅูุดุงุก ูุงุนุฏุฉ ุจูุงูุงุช ุงูุฃุฑุดูู (ุงูุฎูุงุฑ ุงููููุตู ุจู)
CREATE DATABASE kitchen_factory_archive 
CHARACTER SET utf8mb4 
COLLATE utf8mb4_unicode_ci;

-- ุฃู ุฅูุดุงุก ูุฎุทุท ูููุตู (ุงูุฎูุงุฑ ุงูุจุฏูู)
-- CREATE SCHEMA archive;

USE kitchen_factory_archive;
```

#### ุงูุฎุทูุฉ 2: ุฅูุดุงุก ุฌุฏุงูู ุงูุจูุงูุงุช ุงููุตููุฉ
```sql
-- ุฌุฏูู ุงูุจูุงูุงุช ุงููุตููุฉ ุงูุฑุฆูุณู
CREATE TABLE archive_metadata (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    source_table VARCHAR(50) NOT NULL COMMENT 'ุงูุฌุฏูู ุงูุฃุตูู',
    source_id INTEGER NOT NULL COMMENT 'ูุนุฑู ุงูุณุฌู ุงูุฃุตูู',
    archived_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ุชุงุฑูุฎ ุงูุฃุฑุดูุฉ',
    archived_by VARCHAR(100) NOT NULL COMMENT 'ุงููุณุชุฎุฏู ุงูุฐู ูุงู ุจุงูุฃุฑุดูุฉ',
    archive_reason VARCHAR(500) COMMENT 'ุณุจุจ ุงูุฃุฑุดูุฉ',
    archive_type ENUM('manual', 'automatic', 'scheduled') DEFAULT 'automatic' COMMENT 'ููุน ุงูุฃุฑุดูุฉ',
    original_record_json LONGTEXT COMMENT 'ูุณุฎุฉ JSON ูู ุงูุณุฌู ุงูุฃุตูู',
    can_restore BOOLEAN DEFAULT TRUE COMMENT 'ูู ูููู ุงุณุชุนุงุฏุฉ ุงูุณุฌูุ',
    restore_conditions TEXT COMMENT 'ุดุฑูุท ุงูุงุณุชุนุงุฏุฉ',
    data_size_bytes INTEGER COMMENT 'ุญุฌู ุงูุจูุงูุงุช ุงููุคุฑุดูุฉ',
    checksum VARCHAR(64) COMMENT 'ููุชุญูู ูู ุณูุงูุฉ ุงูุจูุงูุงุช',
    expires_at DATETIME COMMENT 'ุชุงุฑูุฎ ุงูุชูุงุก ุตูุงุญูุฉ ุงูุฃุฑุดูู',
    
    -- ุงูููุงุฑุณ
    INDEX idx_source_table_id (source_table, source_id),
    INDEX idx_archived_at (archived_at),
    INDEX idx_archived_by (archived_by),
    INDEX idx_archive_type (archive_type),
    INDEX idx_can_restore (can_restore),
    INDEX idx_expires_at (expires_at)
) COMMENT 'ุฌุฏูู ุงูุจูุงูุงุช ุงููุตููุฉ ูุนูููุงุช ุงูุฃุฑุดูุฉ';

-- ุฌุฏูู ุชุชุจุน ุงูุนูุงูุงุช ุงููุคุฑุดูุฉ
CREATE TABLE archive_relationships (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    parent_table VARCHAR(50) NOT NULL COMMENT 'ุงูุฌุฏูู ุงูุฃุจ',
    parent_id INTEGER NOT NULL COMMENT 'ูุนุฑู ุงูุณุฌู ุงูุฃุจ',
    child_table VARCHAR(50) NOT NULL COMMENT 'ุงูุฌุฏูู ุงููุฑุนู',
    child_id INTEGER NOT NULL COMMENT 'ูุนุฑู ุงูุณุฌู ุงููุฑุนู',
    relationship_type VARCHAR(50) COMMENT 'ููุน ุงูุนูุงูุฉ',
    archived_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    archive_batch_id INTEGER COMMENT 'ูุนุฑู ุฏูุนุฉ ุงูุฃุฑุดูุฉ',
    
    FOREIGN KEY (archive_batch_id) REFERENCES archive_metadata(id) ON DELETE SET NULL,
    INDEX idx_parent (parent_table, parent_id),
    INDEX idx_child (child_table, child_id),
    INDEX idx_batch (archive_batch_id),
    INDEX idx_relationship_type (relationship_type)
) COMMENT 'ุฌุฏูู ุชุชุจุน ุงูุนูุงูุงุช ุจูู ุงูุณุฌูุงุช ุงููุคุฑุดูุฉ';

-- ุฌุฏูู ุฅุญุตุงุฆูุงุช ุงูุฃุฑุดูู
CREATE TABLE archive_statistics (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    table_name VARCHAR(50) NOT NULL UNIQUE COMMENT 'ุงุณู ุงูุฌุฏูู',
    total_archived INTEGER DEFAULT 0 COMMENT 'ุฅุฌูุงูู ุงูุณุฌูุงุช ุงููุคุฑุดูุฉ',
    total_size_mb DECIMAL(10,2) DEFAULT 0 COMMENT 'ุงูุญุฌู ุงูุฅุฌูุงูู ุจุงูููุฌุงุจุงูุช',
    last_archive_date DATETIME COMMENT 'ุชุงุฑูุฎ ุขุฎุฑ ุฃุฑุดูุฉ',
    last_restore_date DATETIME COMMENT 'ุชุงุฑูุฎ ุขุฎุฑ ุงุณุชุนุงุฏุฉ',
    average_archive_age_days INTEGER COMMENT 'ูุชูุณุท ุนูุฑ ุงูุณุฌูุงุช ุงููุคุฑุดูุฉ',
    archive_success_rate DECIMAL(5,2) DEFAULT 100.00 COMMENT 'ูุนุฏู ูุฌุงุญ ุงูุฃุฑุดูุฉ',
    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_table_name (table_name),
    INDEX idx_last_archive (last_archive_date),
    INDEX idx_success_rate (archive_success_rate)
) COMMENT 'ุฅุญุตุงุฆูุงุช ุงูุฃุฑุดูุฉ ููู ุฌุฏูู';

-- ุฌุฏูู ุณุฌู ุนูููุงุช ุงูุฃุฑุดูุฉ
CREATE TABLE archive_operations_log (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    operation_type ENUM('archive', 'restore', 'delete', 'verify', 'search') NOT NULL,
    table_name VARCHAR(50) COMMENT 'ุงุณู ุงูุฌุฏูู ุงููุชุฃุซุฑ',
    record_count INTEGER DEFAULT 0 COMMENT 'ุนุฏุฏ ุงูุณุฌูุงุช ุงููุชุฃุซุฑุฉ',
    operation_start DATETIME DEFAULT CURRENT_TIMESTAMP,
    operation_end DATETIME,
    duration_seconds INTEGER COMMENT 'ูุฏุฉ ุงูุนูููุฉ ุจุงูุซูุงูู',
    status ENUM('running', 'completed', 'failed', 'cancelled') DEFAULT 'running',
    error_message TEXT COMMENT 'ุฑุณุงูุฉ ุงูุฎุทุฃ ูู ุญุงูุฉ ุงููุดู',
    performed_by VARCHAR(100) COMMENT 'ูู ูุงู ุจุงูุนูููุฉ',
    batch_size INTEGER COMMENT 'ุญุฌู ุงูุฏูุนุฉ',
    affected_records TEXT COMMENT 'ูุงุฆูุฉ ูุนุฑูุงุช ุงูุณุฌูุงุช ุงููุชุฃุซุฑุฉ (JSON)',
    performance_metrics JSON COMMENT 'ููุงููุณ ุงูุฃุฏุงุก',
    
    INDEX idx_operation_type (operation_type),
    INDEX idx_operation_start (operation_start),
    INDEX idx_status (status),
    INDEX idx_table_name (table_name),
    INDEX idx_performed_by (performed_by)
) COMMENT 'ุณุฌู ุฌููุน ุนูููุงุช ุงูุฃุฑุดูุฉ ูุงูุงุณุชุนุงุฏุฉ';

-- ุฌุฏูู ุฌุฏููุฉ ุงูุฃุฑุดูุฉ ุงูุชููุงุฆูุฉ
CREATE TABLE archive_scheduler (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    table_name VARCHAR(50) NOT NULL COMMENT 'ุงูุฌุฏูู ุงููุฑุงุฏ ุฃุฑุดูุชู',
    schedule_name VARCHAR(100) NOT NULL COMMENT 'ุงุณู ุงูุฌุฏููุฉ',
    is_enabled BOOLEAN DEFAULT TRUE COMMENT 'ูู ุงูุฌุฏููุฉ ููุนูุฉุ',
    cron_expression VARCHAR(100) COMMENT 'ุชุนุจูุฑ ุงูููุช (ูุซู cron)',
    archive_condition TEXT NOT NULL COMMENT 'ุดุฑุท SQL ููุฃุฑุดูุฉ',
    batch_size INTEGER DEFAULT 100 COMMENT 'ุญุฌู ุงูุฏูุนุฉ',
    max_records_per_run INTEGER DEFAULT 1000 COMMENT 'ุงูุญุฏ ุงูุฃูุตู ููุณุฌูุงุช ูู ุงูุชุดุบูู ุงููุงุญุฏ',
    last_run DATETIME COMMENT 'ุขุฎุฑ ุชุดุบูู',
    next_run DATETIME COMMENT 'ุงูุชุดุบูู ุงูุชุงูู',
    success_count INTEGER DEFAULT 0 COMMENT 'ุนุฏุฏ ุงูุชุดุบููุงุช ุงููุงุฌุญุฉ',
    failure_count INTEGER DEFAULT 0 COMMENT 'ุนุฏุฏ ุงูุชุดุบููุงุช ุงููุงุดูุฉ',
    last_error TEXT COMMENT 'ุขุฎุฑ ุฑุณุงูุฉ ุฎุทุฃ',
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    created_by VARCHAR(100),
    
    INDEX idx_table_name (table_name),
    INDEX idx_enabled (is_enabled),
    INDEX idx_next_run (next_run),
    INDEX idx_schedule_name (schedule_name)
) COMMENT 'ุฌุฏุงูู ุงูุฃุฑุดูุฉ ุงูุชููุงุฆูุฉ ุงููุฌุฏููุฉ';
```

#### ุงูุฎุทูุฉ 3: ุฅูุดุงุก ุฌุฏุงูู ุงูุจูุงูุงุช ุงููุคุฑุดูุฉ
```sql
-- ุฌุฏุงูู ุงูุทูุจูุงุช ุงููุคุฑุดูุฉ
CREATE TABLE archive_orders LIKE kitchen_factory.orders;
ALTER TABLE archive_orders ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_orders ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;
ALTER TABLE archive_orders ADD INDEX idx_archive_metadata (archive_metadata_id);
ALTER TABLE archive_orders ADD INDEX idx_archived_at (archived_at);

CREATE TABLE archive_stages LIKE kitchen_factory.stages;
ALTER TABLE archive_stages ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_stages ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_order_costs LIKE kitchen_factory.order_costs;
ALTER TABLE archive_order_costs ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_order_costs ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_order_material LIKE kitchen_factory.order_material;
ALTER TABLE archive_order_material ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_order_material ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_documents LIKE kitchen_factory.documents;
ALTER TABLE archive_documents ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_documents ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_received_order LIKE kitchen_factory.received_order;
ALTER TABLE archive_received_order ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_received_order ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_payments LIKE kitchen_factory.payments;
ALTER TABLE archive_payments ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_payments ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

-- ุฌุฏุงูู ุงูููุงุชูุฑ ุงููุคุฑุดูุฉ
CREATE TABLE archive_purchase_invoice LIKE kitchen_factory.purchase_invoice;
ALTER TABLE archive_purchase_invoice ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_purchase_invoice ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_purchase_invoice_item LIKE kitchen_factory.purchase_invoice_item;
ALTER TABLE archive_purchase_invoice_item ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_purchase_invoice_item ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_supplier_payment LIKE kitchen_factory.supplier_payment;
ALTER TABLE archive_supplier_payment ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_supplier_payment ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

-- ุฌุฏุงูู ุงูููููู ุงููุคุฑุดูุฉ
CREATE TABLE archive_technician_due LIKE kitchen_factory.technician_due;
ALTER TABLE archive_technician_due ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_technician_due ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_technician_payment LIKE kitchen_factory.technician_payment;
ALTER TABLE archive_technician_payment ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_technician_payment ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

-- ุฌุฏุงูู ุงูุณุฌูุงุช ุงููุคุฑุดูุฉ
CREATE TABLE archive_audit_log LIKE kitchen_factory.audit_log;
ALTER TABLE archive_audit_log ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_audit_log ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_material_consumption LIKE kitchen_factory.material_consumption;
ALTER TABLE archive_material_consumption ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_material_consumption ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;
```

---

## ๐ง ุงููุฑุญูุฉ ุงูุซุงูุซุฉ: ุชุทููุฑ ุงูููุฏ ุงูุฃุณุงุณู

### ๐ ุฅุถุงูุฉ ุงูููุงุฐุฌ ูุงูุฏูุงู ูู Flask

#### ุงูุฎุทูุฉ 1: ุฅุถุงูุฉ ููุงุฐุฌ ุงูุฃุฑุดูุฉ ุฅูู app.py
```python
# ุฅุถุงูุฉ ูุฐุง ุงูููุฏ ูู app.py ุจุนุฏ ุงูููุงุฐุฌ ุงูููุฌูุฏุฉ

# ==================== ููุงุฐุฌ ูุธุงู ุงูุฃุฑุดูุฉ ====================

class ArchiveMetadata(db.Model):
    """ูููุฐุฌ ุงูุจูุงูุงุช ุงููุตููุฉ ููุฃุฑุดูุฉ"""
    __tablename__ = 'archive_metadata'
    __bind_key__ = 'archive'  # ุฑุจุท ุจูุงุนุฏุฉ ุจูุงูุงุช ุงูุฃุฑุดูู
    
    id = db.Column(db.Integer, primary_key=True)
    source_table = db.Column(db.String(50), nullable=False)
    source_id = db.Column(db.Integer, nullable=False)
    archived_at = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    archived_by = db.Column(db.String(100), nullable=False)
    archive_reason = db.Column(db.String(500))
    archive_type = db.Column(db.Enum('manual', 'automatic', 'scheduled'), default='automatic')
    original_record_json = db.Column(db.Text)
    can_restore = db.Column(db.Boolean, default=True)
    restore_conditions = db.Column(db.Text)
    data_size_bytes = db.Column(db.Integer)
    checksum = db.Column(db.String(64))
    expires_at = db.Column(db.DateTime)


class ArchiveRelationship(db.Model):
    """ูููุฐุฌ ุงูุนูุงูุงุช ุงููุคุฑุดูุฉ"""
    __tablename__ = 'archive_relationships'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    parent_table = db.Column(db.String(50), nullable=False)
    parent_id = db.Column(db.Integer, nullable=False)
    child_table = db.Column(db.String(50), nullable=False)
    child_id = db.Column(db.Integer, nullable=False)
    relationship_type = db.Column(db.String(50))
    archived_at = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    archive_batch_id = db.Column(db.Integer, db.ForeignKey('archive_metadata.id'))


class ArchiveStatistics(db.Model):
    """ูููุฐุฌ ุฅุญุตุงุฆูุงุช ุงูุฃุฑุดูุฉ"""
    __tablename__ = 'archive_statistics'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    table_name = db.Column(db.String(50), nullable=False, unique=True)
    total_archived = db.Column(db.Integer, default=0)
    total_size_mb = db.Column(db.Numeric(10, 2), default=0)
    last_archive_date = db.Column(db.DateTime)
    last_restore_date = db.Column(db.DateTime)
    average_archive_age_days = db.Column(db.Integer)
    archive_success_rate = db.Column(db.Numeric(5, 2), default=100.00)
    last_updated = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))


class ArchiveOperationsLog(db.Model):
    """ูููุฐุฌ ุณุฌู ุนูููุงุช ุงูุฃุฑุดูุฉ"""
    __tablename__ = 'archive_operations_log'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    operation_type = db.Column(db.Enum('archive', 'restore', 'delete', 'verify', 'search'), nullable=False)
    table_name = db.Column(db.String(50))
    record_count = db.Column(db.Integer, default=0)
    operation_start = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    operation_end = db.Column(db.DateTime)
    duration_seconds = db.Column(db.Integer)
    status = db.Column(db.Enum('running', 'completed', 'failed', 'cancelled'), default='running')
    error_message = db.Column(db.Text)
    performed_by = db.Column(db.String(100))
    batch_size = db.Column(db.Integer)
    affected_records = db.Column(db.Text)  # JSON string
    performance_metrics = db.Column(db.JSON)


class ArchiveScheduler(db.Model):
    """ูููุฐุฌ ุฌุฏููุฉ ุงูุฃุฑุดูุฉ ุงูุชููุงุฆูุฉ"""
    __tablename__ = 'archive_scheduler'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    table_name = db.Column(db.String(50), nullable=False)
    schedule_name = db.Column(db.String(100), nullable=False)
    is_enabled = db.Column(db.Boolean, default=True)
    cron_expression = db.Column(db.String(100))
    archive_condition = db.Column(db.Text, nullable=False)
    batch_size = db.Column(db.Integer, default=100)
    max_records_per_run = db.Column(db.Integer, default=1000)
    last_run = db.Column(db.DateTime)
    next_run = db.Column(db.DateTime)
    success_count = db.Column(db.Integer, default=0)
    failure_count = db.Column(db.Integer, default=0)
    last_error = db.Column(db.Text)
    created_at = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    created_by = db.Column(db.String(100))
```

#### ุงูุฎุทูุฉ 2: ุฅุถุงูุฉ ุฏูุงู ุงูุฃุฑุดูุฉ ุงูุฃุณุงุณูุฉ
```python
# ==================== ุฏูุงู ูุธุงู ุงูุฃุฑุดูุฉ ====================

import hashlib
import json
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import logging

# ุฅุนุฏุงุฏ ุงูุณุฌู ุงููุฎุตุต ููุฃุฑุดูุฉ
archive_logger = logging.getLogger('archive')

def get_archive_setting(key: str, default: Any = None, value_type: str = 'string') -> Any:
    """ุฌูุจ ุฅุนุฏุงุฏ ูุธุงู ุงูุฃุฑุดูุฉ"""
    return get_system_setting(key, default, value_type)

def calculate_data_checksum(data: dict) -> str:
    """ุญุณุงุจ checksum ููุจูุงูุงุช ููุชุญูู ูู ุงูุณูุงูุฉ"""
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(json_str.encode('utf-8')).hexdigest()

def log_archive_operation(operation_type: str, table_name: str = None, 
                         record_count: int = 0, performed_by: str = None,
                         batch_size: int = None) -> ArchiveOperationsLog:
    """ุชุณุฌูู ุนูููุฉ ุฃุฑุดูุฉ ูู ุงูุณุฌู"""
    operation = ArchiveOperationsLog(
        operation_type=operation_type,
        table_name=table_name,
        record_count=record_count,
        performed_by=performed_by or (current_user.username if current_user.is_authenticated else 'system'),
        batch_size=batch_size,
        status='running'
    )
    
    db.session.add(operation)
    db.session.commit()
    return operation

def complete_archive_operation(operation: ArchiveOperationsLog, 
                             status: str = 'completed',
                             error_message: str = None,
                             affected_records: List = None,
                             performance_metrics: Dict = None):
    """ุฅููุงู ุนูููุฉ ุฃุฑุดูุฉ ูุชุญุฏูุซ ุงูุณุฌู"""
    operation.operation_end = datetime.now(timezone.utc)
    operation.duration_seconds = int((operation.operation_end - operation.operation_start).total_seconds())
    operation.status = status
    operation.error_message = error_message
    
    if affected_records:
        operation.affected_records = json.dumps(affected_records)
    
    if performance_metrics:
        operation.performance_metrics = performance_metrics
    
    db.session.commit()

def update_archive_statistics(table_name: str, archived_count: int = 0, 
                            size_mb: float = 0, operation_type: str = 'archive'):
    """ุชุญุฏูุซ ุฅุญุตุงุฆูุงุช ุงูุฃุฑุดูุฉ"""
    stats = ArchiveStatistics.query.filter_by(table_name=table_name).first()
    
    if not stats:
        stats = ArchiveStatistics(
            table_name=table_name,
            total_archived=0,
            total_size_mb=0,
            archive_success_rate=100.00
        )
        db.session.add(stats)
    
    if operation_type == 'archive':
        stats.total_archived += archived_count
        stats.total_size_mb += size_mb
        stats.last_archive_date = datetime.now(timezone.utc)
    elif operation_type == 'restore':
        stats.total_archived = max(0, stats.total_archived - archived_count)
        stats.last_restore_date = datetime.now(timezone.utc)
    
    stats.last_updated = datetime.now(timezone.utc)
    db.session.commit()

def create_archive_metadata(source_table: str, source_id: int, 
                          original_record: dict, archive_reason: str,
                          archive_type: str = 'automatic') -> ArchiveMetadata:
    """ุฅูุดุงุก ุจูุงูุงุช ูุตููุฉ ููุฃุฑุดูุฉ"""
    
    # ุชุญููู ุงูุจูุงูุงุช ุฅูู JSON ูุญุณุงุจ ุงูchecksum
    json_data = json.dumps(original_record, default=str, ensure_ascii=False)
    checksum = calculate_data_checksum(original_record)
    data_size = len(json_data.encode('utf-8'))
    
    # ุชุญุฏูุฏ ุชุงุฑูุฎ ุงูุชูุงุก ุงูุตูุงุญูุฉ
    retention_years = get_archive_setting('archive_retention_years', 7, 'integer')
    expires_at = datetime.now(timezone.utc).replace(year=datetime.now().year + retention_years)
    
    metadata = ArchiveMetadata(
        source_table=source_table,
        source_id=source_id,
        archived_by=current_user.username if current_user.is_authenticated else 'system',
        archive_reason=archive_reason,
        archive_type=archive_type,
        original_record_json=json_data,
        data_size_bytes=data_size,
        checksum=checksum,
        expires_at=expires_at
    )
    
    db.session.add(metadata)
    db.session.flush()  # ููุญุตูู ุนูู ุงููุนุฑู
    return metadata

def archive_single_record(source_table: str, record_id: int, 
                        archive_reason: str = 'ุชููุงุฆูุฉ',
                        archive_type: str = 'automatic') -> bool:
    """ุฃุฑุดูุฉ ุณุฌู ูุงุญุฏ ูุน ุฌููุน ุงูุจูุงูุงุช ุงููุฑุชุจุทุฉ"""
    
    try:
        archive_logger.info(f"ุจุฏุก ุฃุฑุดูุฉ ุณุฌู {record_id} ูู ุฌุฏูู {source_table}")
        
        # ุจุฏุก ุนูููุฉ ุงูุฃุฑุดูุฉ ูู ุงูุณุฌู
        operation = log_archive_operation('archive', source_table, 1)
        
        # ุฌูุจ ุงูุณุฌู ุงูุฃุตูู
        original_record = get_record_by_table_and_id(source_table, record_id)
        if not original_record:
            raise Exception(f"ุงูุณุฌู {record_id} ุบูุฑ ููุฌูุฏ ูู ุฌุฏูู {source_table}")
        
        # ุฅูุดุงุก ุงูุจูุงูุงุช ุงููุตููุฉ
        metadata = create_archive_metadata(
            source_table, record_id, original_record, archive_reason, archive_type
        )
        
        # ุฃุฑุดูุฉ ุงูุณุฌู ุงูุฑุฆูุณู
        archive_main_record(source_table, original_record, metadata.id)
        
        # ุฃุฑุดูุฉ ุงูุณุฌูุงุช ุงููุฑุชุจุทุฉ
        related_count = archive_related_records(source_table, record_id, metadata.id)
        
        # ุญุฐู ุงูุณุฌูุงุช ูู ุงูุฌุฏุงูู ุงูุฃุตููุฉ
        delete_original_records(source_table, record_id)
        
        # ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช
        update_archive_statistics(source_table, 1, metadata.data_size_bytes / (1024 * 1024))
        
        # ุฅููุงู ุงูุนูููุฉ
        complete_archive_operation(
            operation, 
            'completed',
            affected_records=[record_id],
            performance_metrics={
                'main_record': 1,
                'related_records': related_count,
                'data_size_mb': metadata.data_size_bytes / (1024 * 1024)
            }
        )
        
        archive_logger.info(f"ุชู ุฃุฑุดูุฉ ุงูุณุฌู {record_id} ุจูุฌุงุญ")
        return True
        
    except Exception as e:
        archive_logger.error(f"ูุดู ุฃุฑุดูุฉ ุงูุณุฌู {record_id}: {str(e)}")
        complete_archive_operation(operation, 'failed', str(e))
        db.session.rollback()
        return False

def get_record_by_table_and_id(table_name: str, record_id: int) -> dict:
    """ุฌูุจ ุณุฌู ูู ุฌุฏูู ูุญุฏุฏ ุจูุงุกู ุนูู ุงููุนุฑู"""
    
    # ุฎุฑูุทุฉ ุงูุฌุฏุงูู ูุงูููุงุฐุฌ
    table_models = {
        'orders': Order,
        'purchase_invoice': PurchaseInvoice,
        'technician_due': TechnicianDue,
        'audit_log': AuditLog,
        'material_consumption': MaterialConsumption
    }
    
    model_class = table_models.get(table_name)
    if not model_class:
        raise Exception(f"ุฌุฏูู ุบูุฑ ูุฏุนูู: {table_name}")
    
    record = model_class.query.get(record_id)
    if not record:
        return None
    
    # ุชุญููู ุงูุณุฌู ุฅูู dictionary
    return {column.name: getattr(record, column.name) 
            for column in model_class.__table__.columns}

def archive_main_record(source_table: str, record_data: dict, metadata_id: int):
    """ุฃุฑุดูุฉ ุงูุณุฌู ุงูุฑุฆูุณู"""
    
    # ุฎุฑูุทุฉ ุฌุฏุงูู ุงูุฃุฑุดูุฉ
    archive_tables = {
        'orders': 'archive_orders',
        'purchase_invoice': 'archive_purchase_invoice', 
        'technician_due': 'archive_technician_due',
        'audit_log': 'archive_audit_log',
        'material_consumption': 'archive_material_consumption'
    }
    
    archive_table = archive_tables.get(source_table)
    if not archive_table:
        raise Exception(f"ุฌุฏูู ุฃุฑุดูู ุบูุฑ ููุฌูุฏ ูู: {source_table}")
    
    # ุฅุถุงูุฉ ูุนูููุงุช ุงูุฃุฑุดูุฉ
    record_data['archive_metadata_id'] = metadata_id
    record_data['archived_at'] = datetime.now(timezone.utc)
    
    # ุฅุฏุฑุงุฌ ุงูุณุฌู ูู ุฌุฏูู ุงูุฃุฑุดูู
    columns = ', '.join(record_data.keys())
    placeholders = ', '.join(['%s'] * len(record_data))
    
    query = f"INSERT INTO {archive_table} ({columns}) VALUES ({placeholders})"
    
    # ุชูููุฐ ุงูุงุณุชุนูุงู ูุน ูุงุนุฏุฉ ุจูุงูุงุช ุงูุฃุฑุดูู
    archive_db = db.get_engine(app, 'archive')
    archive_db.execute(query, list(record_data.values()))

def archive_related_records(source_table: str, parent_id: int, metadata_id: int) -> int:
    """ุฃุฑุดูุฉ ุงูุณุฌูุงุช ุงููุฑุชุจุทุฉ ุจุงูุณุฌู ุงูุฑุฆูุณู"""
    
    # ููุงุนุฏ ุงูุนูุงูุงุช ููุฌุฏุงูู
    relationships_config = {
        'orders': [
            {'table': 'stages', 'foreign_key': 'order_id', 'archive_table': 'archive_stages'},
            {'table': 'order_costs', 'foreign_key': 'order_id', 'archive_table': 'archive_order_costs'},
            {'table': 'order_material', 'foreign_key': 'order_id', 'archive_table': 'archive_order_material'},
            {'table': 'payments', 'foreign_key': 'order_id', 'archive_table': 'archive_payments'},
            {'table': 'documents', 'foreign_key': 'order_id', 'archive_table': 'archive_documents'},
            {'table': 'received_order', 'foreign_key': 'order_id', 'archive_table': 'archive_received_order'}
        ],
        'purchase_invoice': [
            {'table': 'purchase_invoice_item', 'foreign_key': 'invoice_id', 'archive_table': 'archive_purchase_invoice_item'},
            {'table': 'supplier_payment', 'foreign_key': 'invoice_id', 'archive_table': 'archive_supplier_payment'}
        ]
    }
    
    relationships = relationships_config.get(source_table, [])
    total_archived = 0
    
    for rel in relationships:
        related_records = db.session.execute(
            f"SELECT * FROM {rel['table']} WHERE {rel['foreign_key']} = %s",
            [parent_id]
        ).fetchall()
        
        for record in related_records:
            # ุชุญููู ุงูุณุฌู ุฅูู dictionary
            record_dict = dict(record)
            record_dict['archive_metadata_id'] = metadata_id
            record_dict['archived_at'] = datetime.now(timezone.utc)
            
            # ุฅุฏุฑุงุฌ ูู ุฌุฏูู ุงูุฃุฑุดูู
            columns = ', '.join(record_dict.keys())
            placeholders = ', '.join(['%s'] * len(record_dict))
            
            query = f"INSERT INTO {rel['archive_table']} ({columns}) VALUES ({placeholders})"
            archive_db = db.get_engine(app, 'archive')
            archive_db.execute(query, list(record_dict.values()))
            
            # ุชุณุฌูู ุงูุนูุงูุฉ
            relationship = ArchiveRelationship(
                parent_table=source_table,
                parent_id=parent_id,
                child_table=rel['table'],
                child_id=record['id'],
                relationship_type='one_to_many',
                archive_batch_id=metadata_id
            )
            db.session.add(relationship)
            
            total_archived += 1
    
    return total_archived

def delete_original_records(source_table: str, parent_id: int):
    """ุญุฐู ุงูุณุฌูุงุช ูู ุงูุฌุฏุงูู ุงูุฃุตููุฉ ุจุนุฏ ุงูุฃุฑุดูุฉ"""
    
    # ุญุฐู ุงูุณุฌูุงุช ุงููุฑุชุจุทุฉ ุฃููุงู (ุจูุงุกู ุนูู ุงูุนูุงูุงุช)
    relationships_config = {
        'orders': ['payments', 'documents', 'received_order', 'order_material', 'order_costs', 'stages'],
        'purchase_invoice': ['supplier_payment', 'purchase_invoice_item'],
        'technician_due': []  # ุณุฌู ูููุฑุฏ
    }
    
    related_tables = relationships_config.get(source_table, [])
    foreign_key_map = {
        'orders': 'order_id',
        'purchase_invoice': 'invoice_id',
        'technician_due': 'due_id'
    }
    
    foreign_key = foreign_key_map.get(source_table, f"{source_table}_id")
    
    # ุญุฐู ุงูุณุฌูุงุช ุงููุฑุชุจุทุฉ
    for table in related_tables:
        db.session.execute(f"DELETE FROM {table} WHERE {foreign_key} = %s", [parent_id])
    
    # ุญุฐู ุงูุณุฌู ุงูุฑุฆูุณู
    db.session.execute(f"DELETE FROM {source_table} WHERE id = %s", [parent_id])
    db.session.commit()
```

#### ุงูุฎุทูุฉ 3: ุฅุถุงูุฉ ุฏูุงู ุงูุฃุฑุดูุฉ ุงููุชูุฏูุฉ
```python
def archive_batch_records(source_table: str, record_ids: List[int], 
                         archive_reason: str = 'ุฏูุนูุฉ', 
                         batch_size: int = None) -> Dict[str, Any]:
    """ุฃุฑุดูุฉ ูุฌููุนุฉ ูู ุงูุณุฌูุงุช ูู ุฏูุนุงุช"""
    
    if not batch_size:
        batch_size = get_archive_setting('archive_batch_size', 50, 'integer')
    
    total_records = len(record_ids)
    successful_count = 0
    failed_count = 0
    failed_records = []
    
    # ุชุณุฌูู ุจุฏุงูุฉ ุงูุนูููุฉ
    operation = log_archive_operation('archive', source_table, total_records)
    
    try:
        # ูุนุงูุฌุฉ ุงูุณุฌูุงุช ูู ุฏูุนุงุช
        for i in range(0, total_records, batch_size):
            batch = record_ids[i:i + batch_size]
            
            for record_id in batch:
                if archive_single_record(source_table, record_id, archive_reason, 'manual'):
                    successful_count += 1
                else:
                    failed_count += 1
                    failed_records.append(record_id)
            
            # ูุงุตู ุฒููู ุจูู ุงูุฏูุนุงุช ูุชุฌูุจ ุงูุญูู ุงูุฒุงุฆุฏ
            import time
            time.sleep(0.1)
        
        # ุฅููุงู ุงูุนูููุฉ
        status = 'completed' if failed_count == 0 else 'partial_failure'
        complete_archive_operation(
            operation,
            status,
            f"ูุดู ูู {failed_count} ูู ุฃุตู {total_records}" if failed_count > 0 else None,
            record_ids,
            {
                'successful_count': successful_count,
                'failed_count': failed_count,
                'failed_records': failed_records,
                'batch_size': batch_size
            }
        )
        
        return {
            'success': True,
            'total_records': total_records,
            'successful_count': successful_count,
            'failed_count': failed_count,
            'failed_records': failed_records
        }
        
    except Exception as e:
        complete_archive_operation(operation, 'failed', str(e))
        return {
            'success': False,
            'error': str(e),
            'successful_count': successful_count,
            'failed_count': failed_count
        }

def auto_archive_eligible_records():
    """ุฃุฑุดูุฉ ุชููุงุฆูุฉ ููุณุฌูุงุช ุงููุคููุฉ ุญุณุจ ุงูุฅุนุฏุงุฏุงุช"""
    
    if not get_archive_setting('archive_system_enabled', True, 'boolean'):
        return {'message': 'ูุธุงู ุงูุฃุฑุดูุฉ ูุนุทู'}
    
    if not get_archive_setting('archive_auto_mode_enabled', True, 'boolean'):
        return {'message': 'ุงูุฃุฑุดูุฉ ุงูุชููุงุฆูุฉ ูุนุทูุฉ'}
    
    results = {}
    
    # ุฃุฑุดูุฉ ุงูุทูุจูุงุช ุงููุคููุฉ
    order_days = get_archive_setting('order_auto_archive_days', 90, 'integer')
    eligible_orders = db.session.execute(f"""
        SELECT id FROM orders 
        WHERE status IN ('ูุณููู', 'ููุชูู')
        AND delivery_date IS NOT NULL
        AND delivery_date <= DATE_SUB(NOW(), INTERVAL {order_days} DAY)
        AND (
            SELECT COUNT(*) FROM payments p 
            WHERE p.order_id = orders.id AND p.amount > 0
        ) > 0
        AND NOT EXISTS (
            SELECT 1 FROM archive_metadata am 
            WHERE am.source_table = 'orders' AND am.source_id = orders.id
        )
        LIMIT {get_archive_setting('archive_max_daily_records', 500, 'integer')}
    """).fetchall()
    
    if eligible_orders:
        order_ids = [row[0] for row in eligible_orders]
        results['orders'] = archive_batch_records('orders', order_ids, 'ุชููุงุฆูุฉ - ุงูุชูุงุก ุงููุฏุฉ ุงููุญุฏุฏุฉ')
    
    # ุฃุฑุดูุฉ ุงูููุงุชูุฑ ุงููุคููุฉ
    invoice_days = get_archive_setting('invoice_auto_archive_days', 120, 'integer')
    eligible_invoices = db.session.execute(f"""
        SELECT id FROM purchase_invoice 
        WHERE status = 'paid'
        AND payment_date IS NOT NULL
        AND payment_date <= DATE_SUB(NOW(), INTERVAL {invoice_days} DAY)
        AND NOT EXISTS (
            SELECT 1 FROM archive_metadata am 
            WHERE am.source_table = 'purchase_invoice' AND am.source_id = purchase_invoice.id
        )
        LIMIT 100
    """).fetchall()
    
    if eligible_invoices:
        invoice_ids = [row[0] for row in eligible_invoices]
        results['invoices'] = archive_batch_records('purchase_invoice', invoice_ids, 'ุชููุงุฆูุฉ - ูุงุชูุฑุฉ ูุฏููุนุฉ')
    
    # ุฃุฑุดูุฉ ูุณุชุญูุงุช ุงูููููู ุงููุคููุฉ
    technician_days = get_archive_setting('technician_payment_archive_days', 180, 'integer')
    eligible_dues = db.session.execute(f"""
        SELECT id FROM technician_due 
        WHERE is_paid = true
        AND paid_at IS NOT NULL
        AND paid_at <= DATE_SUB(NOW(), INTERVAL {technician_days} DAY)
        AND NOT EXISTS (
            SELECT 1 FROM archive_metadata am 
            WHERE am.source_table = 'technician_due' AND am.source_id = technician_due.id
        )
        LIMIT 200
    """).fetchall()
    
    if eligible_dues:
        due_ids = [row[0] for row in eligible_dues]
        results['technician_dues'] = archive_batch_records('technician_due', due_ids, 'ุชููุงุฆูุฉ - ูุณุชุญู ูุฏููุน')
    
    return results

def restore_archived_record(source_table: str, record_id: int, 
                          restore_reason: str = 'ุทูุจ ุงุณุชุนุงุฏุฉ') -> bool:
    """ุงุณุชุนุงุฏุฉ ุณุฌู ูู ุงูุฃุฑุดูู ุฅูู ุงููุธุงู ุงูุฑุฆูุณู"""
    
    try:
        # ุงูุจุญุซ ุนู ุงูุจูุงูุงุช ุงููุตููุฉ
        metadata = ArchiveMetadata.query.filter_by(
            source_table=source_table, 
            source_id=record_id
        ).first()
        
        if not metadata:
            raise Exception(f"ูุง ููุฌุฏ ุณุฌู ูุคุฑุดู ูููุนุฑู {record_id} ูู ุฌุฏูู {source_table}")
        
        if not metadata.can_restore:
            raise Exception(f"ุงูุณุฌู {record_id} ูุง ูููู ุงุณุชุนุงุฏุชู: {metadata.restore_conditions}")
        
        # ุชุณุฌูู ุจุฏุงูุฉ ุนูููุฉ ุงูุงุณุชุนุงุฏุฉ
        operation = log_archive_operation('restore', source_table, 1)
        
        # ุงุณุชุนุงุฏุฉ ุงูุณุฌู ุงูุฑุฆูุณู
        restore_main_record(source_table, record_id, metadata.id)
        
        # ุงุณุชุนุงุฏุฉ ุงูุณุฌูุงุช ุงููุฑุชุจุทุฉ
        restored_count = restore_related_records(source_table, record_id, metadata.id)
        
        # ุญุฐู ุงูุณุฌูุงุช ูู ุงูุฃุฑุดูู
        delete_archived_records(source_table, record_id, metadata.id)
        
        # ุชุญุฏูุซ ุงูุฅุญุตุงุฆูุงุช
        update_archive_statistics(source_table, -1, operation_type='restore')
        
        # ุฅููุงู ุงูุนูููุฉ
        complete_archive_operation(
            operation,
            'completed',
            affected_records=[record_id],
            performance_metrics={
                'main_record': 1,
                'related_records': restored_count,
                'restore_reason': restore_reason
            }
        )
        
        archive_logger.info(f"ุชู ุงุณุชุนุงุฏุฉ ุงูุณุฌู {record_id} ูู {source_table} ุจูุฌุงุญ")
        return True
        
    except Exception as e:
        archive_logger.error(f"ูุดู ุงุณุชุนุงุฏุฉ ุงูุณุฌู {record_id}: {str(e)}")
        complete_archive_operation(operation, 'failed', str(e))
        return False

def search_archived_records(search_params: Dict[str, Any]) -> List[Dict]:
    """ุงูุจุญุซ ูู ุงูุณุฌูุงุช ุงููุคุฑุดูุฉ"""
    
    table_name = search_params.get('table_name')
    start_date = search_params.get('start_date')
    end_date = search_params.get('end_date')
    archive_reason = search_params.get('archive_reason')
    archived_by = search_params.get('archived_by')
    limit = search_params.get('limit', get_archive_setting('archive_search_results_limit', 100, 'integer'))
    
    # ุชุณุฌูู ุนูููุฉ ุงูุจุญุซ
    operation = log_archive_operation('search', table_name)
    
    try:
        # ุจูุงุก ุงุณุชุนูุงู ุงูุจุญุซ
        query = "SELECT * FROM archive_metadata WHERE 1=1"
        params = []
        
        if table_name:
            query += " AND source_table = %s"
            params.append(table_name)
        
        if start_date:
            query += " AND archived_at >= %s"
            params.append(start_date)
        
        if end_date:
            query += " AND archived_at <= %s"
            params.append(end_date)
        
        if archive_reason:
            query += " AND archive_reason LIKE %s"
            params.append(f"%{archive_reason}%")
        
        if archived_by:
            query += " AND archived_by = %s"
            params.append(archived_by)
        
        query += f" ORDER BY archived_at DESC LIMIT {limit}"
        
        # ุชูููุฐ ุงูุจุญุซ
        archive_db = db.get_engine(app, 'archive')
        results = archive_db.execute(query, params).fetchall()
        
        # ุชุญููู ุงููุชุงุฆุฌ ุฅูู ูุงุฆูุฉ ูู ุงูููุงููุณ
        search_results = []
        for row in results:
            result = dict(row)
            # ุฅุถุงูุฉ ุงูุจูุงูุงุช ุงูุฃุตููุฉ ุฅุฐุง ูุงูุช ูุชุงุญุฉ
            if result['original_record_json']:
                result['original_data'] = json.loads(result['original_record_json'])
            search_results.append(result)
        
        # ุฅููุงู ุนูููุฉ ุงูุจุญุซ
        complete_archive_operation(
            operation,
            'completed',
            performance_metrics={
                'results_count': len(search_results),
                'search_params': search_params
            }
        )
        
        return search_results
        
    except Exception as e:
        complete_archive_operation(operation, 'failed', str(e))
        raise e
```

---

## ๐ ุงููุฑุญูุฉ ุงูุฑุงุจุนุฉ: ุชุทููุฑ ุงููุงุฌูุงุช

### ๐ฑ ุฅุถุงูุฉ ูุณุงุฑุงุช ุงูุฃุฑุดูุฉ ุฅูู Flask

#### ุงูุฎุทูุฉ 1: ูุณุงุฑุงุช ุฅุฏุงุฑุฉ ุงูุฃุฑุดูุฉ
```python
# ==================== ูุณุงุฑุงุช ูุธุงู ุงูุฃุฑุดูุฉ ====================

# ุฏูููุฑูุชุฑ ููุชุญูู ุจุตูุงุญูุงุช ุงูุฃุฑุดูู
def require_archive_permission(permission_level='viewer'):
    """ุฏูููุฑูุชุฑ ููุชุญูู ุจุตูุงุญูุงุช ุงููุตูู ููุฃุฑุดูู"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if not current_user.is_authenticated:
                flash('ูุฌุจ ุชุณุฌูู ุงูุฏุฎูู ุฃููุงู', 'danger')
                return redirect(url_for('login'))
            
            # ุชุญุฏูุฏ ุงูุฃุฏูุงุฑ ุงููุณููุญุฉ ููู ูุณุชูู ุตูุงุญูุฉ
            permission_roles = {
                'viewer': ['ูุฏูุฑ', 'ูุณุคูู ุงูุนูููุงุช'],
                'manager': ['ูุฏูุฑ'],
                'admin': ['ูุฏูุฑ']
            }
            
            allowed_roles = permission_roles.get(permission_level, ['ูุฏูุฑ'])
            
            if current_user.role not in allowed_roles:
                flash('ููุณ ูุฏูู ุตูุงุญูุฉ ูููุตูู ุฅูู ูุธุงู ุงูุฃุฑุดูู', 'danger')
                return redirect(url_for('dashboard'))
            
            return func(*args, **kwargs)
        return wrapper
    return decorator

@app.route('/archive')
@login_required
@require_archive_permission('viewer')
def archive_dashboard():
    """ููุญุฉ ูุนูููุงุช ุงูุฃุฑุดูู ุงูุฑุฆูุณูุฉ"""
    
    # ุฌูุน ุงูุฅุญุตุงุฆูุงุช ุงูุนุงูุฉ
    stats = {}
    
    # ุฅุญุตุงุฆูุงุช ุงูุฌุฏุงูู ุงููุคุฑุดูุฉ
    archive_stats = ArchiveStatistics.query.all()
    stats['tables'] = {stat.table_name: {
        'total_archived': stat.total_archived,
        'total_size_mb': float(stat.total_size_mb or 0),
        'last_archive_date': stat.last_archive_date,
        'success_rate': float(stat.archive_success_rate or 100)
    } for stat in archive_stats}
    
    # ุฅุญุตุงุฆูุงุช ุงูุนูููุงุช ุงูุฃุฎูุฑุฉ
    recent_operations = ArchiveOperationsLog.query.order_by(
        ArchiveOperationsLog.operation_start.desc()
    ).limit(10).all()
    
    stats['recent_operations'] = recent_operations
    
    # ุญุณุงุจ ุงูุฅุฌูุงููุงุช
    stats['totals'] = {
        'total_archived_records': sum([stat.total_archived for stat in archive_stats]),
        'total_size_mb': sum([float(stat.total_size_mb or 0) for stat in archive_stats]),
        'active_schedules': ArchiveScheduler.query.filter_by(is_enabled=True).count()
    }
    
    return render_template('archive/dashboard.html', stats=stats)

@app.route('/archive/orders')
@login_required  
@require_archive_permission('viewer')
def archived_orders():
    """ุนุฑุถ ุงูุทูุจูุงุช ุงููุคุฑุดูุฉ"""
    
    page = request.args.get('page', 1, type=int)
    per_page = 20
    
    # ููุงุชุฑ ุงูุจุญุซ
    customer_name = request.args.get('customer_name', '')
    start_date = request.args.get('start_date', '')
    end_date = request.args.get('end_date', '')
    
    # ุจูุงุก ุงุณุชุนูุงู ุงูุจุญุซ
    query = """
        SELECT 
            am.*,
            JSON_EXTRACT(am.original_record_json, '$.customer_name') as customer_name,
            JSON_EXTRACT(am.original_record_json, '$.total_value') as total_value,
            JSON_EXTRACT(am.original_record_json, '$.order_date') as order_date,
            JSON_EXTRACT(am.original_record_json, '$.delivery_date') as delivery_date
        FROM archive_metadata am 
        WHERE am.source_table = 'orders'
    """
    
    params = []
    
    if customer_name:
        query += " AND JSON_EXTRACT(am.original_record_json, '$.customer_name') LIKE %s"
        params.append(f"%{customer_name}%")
    
    if start_date:
        query += " AND am.archived_at >= %s"
        params.append(start_date)
    
    if end_date:
        query += " AND am.archived_at <= %s"
        params.append(end_date)
    
    query += " ORDER BY am.archived_at DESC"
    
    # ุชูููุฐ ุงูุงุณุชุนูุงู ูุน ุงูุชุตูุญ
    offset = (page - 1) * per_page
    paginated_query = f"{query} LIMIT {per_page} OFFSET {offset}"
    
    archive_db = db.get_engine(app, 'archive')
    results = archive_db.execute(paginated_query, params).fetchall()
    
    # ุญุณุงุจ ุฅุฌูุงูู ุงููุชุงุฆุฌ ููุชุตูุญ
    count_query = f"SELECT COUNT(*) FROM ({query}) as count_subquery"
    total = archive_db.execute(count_query, params).scalar()
    
    # ุฅูุดุงุก ูุงุฆู ุงูุชุตูุญ
    pagination = {
        'page': page,
        'per_page': per_page,
        'total': total,
        'pages': (total + per_page - 1) // per_page,
        'has_prev': page > 1,
        'has_next': page * per_page < total,
        'prev_num': page - 1 if page > 1 else None,
        'next_num': page + 1 if page * per_page < total else None
    }
    
    return render_template('archive/orders.html', 
                         results=results, 
                         pagination=pagination,
                         filters={'customer_name': customer_name, 
                                'start_date': start_date, 
                                'end_date': end_date})

@app.route('/archive/search', methods=['GET', 'POST'])
@login_required
@require_archive_permission('viewer')
def archive_search():
    """ุงูุจุญุซ ุงููุชูุฏู ูู ุงูุฃุฑุดูู"""
    
    results = []
    search_params = {}
    
    if request.method == 'POST':
        search_params = {
            'table_name': request.form.get('table_name'),
            'start_date': request.form.get('start_date'),
            'end_date': request.form.get('end_date'),
            'archive_reason': request.form.get('archive_reason'),
            'archived_by': request.form.get('archived_by'),
            'limit': int(request.form.get('limit', 100))
        }
        
        # ุฅุฒุงูุฉ ุงูููู ุงููุงุฑุบุฉ
        search_params = {k: v for k, v in search_params.items() if v}
        
        try:
            results = search_archived_records(search_params)
            flash(f'ุชู ุงูุนุซูุฑ ุนูู {len(results)} ูุชูุฌุฉ', 'success')
        except Exception as e:
            flash(f'ุญุฏุซ ุฎุทุฃ ูู ุงูุจุญุซ: {str(e)}', 'danger')
    
    # ูุงุฆูุฉ ุงูุฌุฏุงูู ุงููุชุงุญุฉ ููุจุญุซ
    available_tables = [
        {'value': 'orders', 'label': 'ุงูุทูุจูุงุช'},
        {'value': 'purchase_invoice', 'label': 'ููุงุชูุฑ ุงูุดุฑุงุก'},
        {'value': 'technician_due', 'label': 'ูุณุชุญูุงุช ุงูููููู'},
        {'value': 'audit_log', 'label': 'ุณุฌู ุงูุชุฏููู'},
        {'value': 'material_consumption', 'label': 'ุงุณุชููุงู ุงูููุงุฏ'}
    ]
    
    return render_template('archive/search.html', 
                         results=results,
                         search_params=search_params,
                         available_tables=available_tables)

@app.route('/archive/restore/<table_name>/<int:record_id>', methods=['POST'])
@login_required
@require_archive_permission('manager')
def restore_archived_record_route(table_name, record_id):
    """ุงุณุชุนุงุฏุฉ ุณุฌู ูู ุงูุฃุฑุดูู"""
    
    restore_reason = request.form.get('restore_reason', 'ุทูุจ ุงุณุชุนุงุฏุฉ ูู ุงููุณุชุฎุฏู')
    
    try:
        success = restore_archived_record(table_name, record_id, restore_reason)
        
        if success:
            flash(f'ุชู ุงุณุชุนุงุฏุฉ ุงูุณุฌู ุฑูู {record_id} ูู {table_name} ุจูุฌุงุญ', 'success')
            
            # ุชุณุฌูู ูู ุณุฌู ุงูุชุฏููู
            add_audit_log(
                action_type='ุงุณุชุนุงุฏุฉ ูู ุงูุฃุฑุดูู',
                entity_type=table_name,
                entity_id=record_id,
                details=f'ุชู ุงุณุชุนุงุฏุฉ ุงูุณุฌู ูู ุงูุฃุฑุดูู. ุงูุณุจุจ: {restore_reason}',
                user_id=current_user.id
            )
        else:
            flash(f'ูุดู ูู ุงุณุชุนุงุฏุฉ ุงูุณุฌู ุฑูู {record_id}', 'danger')
    
    except Exception as e:
        flash(f'ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุงูุงุณุชุนุงุฏุฉ: {str(e)}', 'danger')
    
    return redirect(request.referrer or url_for('archive_dashboard'))

@app.route('/archive/manual/<table_name>/<int:record_id>', methods=['POST'])
@login_required
@require_archive_permission('manager')
def manual_archive_record(table_name, record_id):
    """ุฃุฑุดูุฉ ุณุฌู ูุฏููุงู"""
    
    archive_reason = request.form.get('archive_reason', 'ุฃุฑุดูุฉ ูุฏููุฉ')
    
    try:
        success = archive_single_record(table_name, record_id, archive_reason, 'manual')
        
        if success:
            flash(f'ุชู ุฃุฑุดูุฉ ุงูุณุฌู ุฑูู {record_id} ูู {table_name} ุจูุฌุงุญ', 'success')
            
            # ุชุณุฌูู ูู ุณุฌู ุงูุชุฏููู
            add_audit_log(
                action_type='ุฃุฑุดูุฉ ูุฏููุฉ',
                entity_type=table_name,
                entity_id=record_id,
                details=f'ุชู ุฃุฑุดูุฉ ุงูุณุฌู ูุฏููุงู. ุงูุณุจุจ: {archive_reason}',
                user_id=current_user.id
            )
        else:
            flash(f'ูุดู ูู ุฃุฑุดูุฉ ุงูุณุฌู ุฑูู {record_id}', 'danger')
    
    except Exception as e:
        flash(f'ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุงูุฃุฑุดูุฉ: {str(e)}', 'danger')
    
    return redirect(request.referrer or url_for('dashboard'))

@app.route('/archive/bulk-archive', methods=['GET', 'POST'])
@login_required
@require_archive_permission('manager')
def bulk_archive():
    """ุฃุฑุดูุฉ ุฌูุงุนูุฉ ููุณุฌูุงุช"""
    
    if request.method == 'POST':
        table_name = request.form.get('table_name')
        record_ids = request.form.getlist('record_ids')
        archive_reason = request.form.get('archive_reason', 'ุฃุฑุดูุฉ ุฌูุงุนูุฉ')
        
        if not record_ids:
            flash('ูู ูุชู ุงุฎุชูุงุฑ ุฃู ุณุฌูุงุช ููุฃุฑุดูุฉ', 'warning')
            return redirect(request.referrer)
        
        # ุชุญููู ูุนุฑูุงุช ุงูุณุฌูุงุช ุฅูู ุฃุฑูุงู
        try:
            record_ids = [int(rid) for rid in record_ids]
        except ValueError:
            flash('ูุนุฑูุงุช ุงูุณุฌูุงุช ุบูุฑ ุตุญูุญุฉ', 'danger')
            return redirect(request.referrer)
        
        # ุชูููุฐ ุงูุฃุฑุดูุฉ ุงูุฌูุงุนูุฉ
        result = archive_batch_records(table_name, record_ids, archive_reason)
        
        if result['success']:
            flash(f'ุชู ุฃุฑุดูุฉ {result["successful_count"]} ุณุฌู ุจูุฌุงุญ ูู ุฃุตู {result["total_records"]}', 'success')
            
            if result['failed_count'] > 0:
                flash(f'ูุดู ูู ุฃุฑุดูุฉ {result["failed_count"]} ุณุฌู', 'warning')
        else:
            flash(f'ูุดู ูู ุงูุนูููุฉ: {result.get("error", "ุฎุทุฃ ุบูุฑ ูุนุฑูู")}', 'danger')
        
        return redirect(request.referrer or url_for('archive_dashboard'))
    
    return render_template('archive/bulk_archive.html')

@app.route('/archive/auto-archive', methods=['POST'])
@login_required
@require_archive_permission('admin')
def trigger_auto_archive():
    """ุชุดุบูู ุงูุฃุฑุดูุฉ ุงูุชููุงุฆูุฉ ูุฏููุงู"""
    
    try:
        results = auto_archive_eligible_records()
        
        total_archived = 0
        messages = []
        
        for table, result in results.items():
            if isinstance(result, dict) and result.get('success'):
                count = result['successful_count']
                total_archived += count
                messages.append(f'{table}: {count} ุณุฌู')
        
        if total_archived > 0:
            flash(f'ุชู ุฃุฑุดูุฉ {total_archived} ุณุฌู: {", ".join(messages)}', 'success')
        else:
            flash('ูุง ุชูุฌุฏ ุณุฌูุงุช ูุคููุฉ ููุฃุฑุดูุฉ ุญุงููุงู', 'info')
    
    except Exception as e:
        flash(f'ุญุฏุซ ุฎุทุฃ ูู ุงูุฃุฑุดูุฉ ุงูุชููุงุฆูุฉ: {str(e)}', 'danger')
    
    return redirect(url_for('archive_dashboard'))

@app.route('/archive/statistics')
@login_required
@require_archive_permission('viewer')
def archive_statistics():
    """ุนุฑุถ ุฅุญุตุงุฆูุงุช ููุตูุฉ ููุฃุฑุดูู"""
    
    # ุฅุญุตุงุฆูุงุช ุดุงููุฉ
    stats = {}
    
    # ุฅุญุตุงุฆูุงุช ุงูุฌุฏุงูู
    table_stats = ArchiveStatistics.query.all()
    stats['tables'] = table_stats
    
    # ุฅุญุตุงุฆูุงุช ุงูุนูููุงุช (ุขุฎุฑ 30 ููู)
    thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
    
    operation_stats = db.session.execute("""
        SELECT 
            operation_type,
            status,
            COUNT(*) as count,
            AVG(duration_seconds) as avg_duration,
            SUM(record_count) as total_records
        FROM archive_operations_log 
        WHERE operation_start >= %s
        GROUP BY operation_type, status
    """, [thirty_days_ago]).fetchall()
    
    stats['operations'] = operation_stats
    
    # ุฅุญุตุงุฆูุงุช ุงูุงุณุชุฎุฏุงู (ุงูุจุญุซ ูุงูุงุณุชุนุงุฏุฉ)
    usage_stats = db.session.execute("""
        SELECT 
            DATE(operation_start) as date,
            operation_type,
            COUNT(*) as count
        FROM archive_operations_log 
        WHERE operation_start >= %s
        AND operation_type IN ('search', 'restore')
        GROUP BY DATE(operation_start), operation_type
        ORDER BY date DESC
    """, [thirty_days_ago]).fetchall()
    
    stats['usage'] = usage_stats
    
    # ุญุฌู ุงูุฃุฑุดูู ููุงุจู ุงููุธุงู ุงูุฑุฆูุณู
    main_db_size = db.session.execute("""
        SELECT 
            table_name,
            ROUND(((data_length + index_length) / 1024 / 1024), 2) AS size_mb,
            table_rows
        FROM information_schema.tables 
        WHERE table_schema = DATABASE()
        AND table_name IN ('orders', 'purchase_invoice', 'technician_due', 'audit_log')
    """).fetchall()
    
    stats['size_comparison'] = main_db_size
    
    return render_template('archive/statistics.html', stats=stats)
```

---

ูุฐุง ูุงู **ุงูุฌุฒุก ุงูุซุงูู** ูู ุฎุทุฉ ูุธุงู ุงูุฃุฑุดูุฉ ุงูุดุงููุ ูุงูุฐู ุฑูุฒ ุนูู **ุงูุชุทุจูู ุงูุชููู**. 

**๐ ููุงุญุธุฉ ูููุฉ:** ูุฐุง ุงูุฌุฒุก ูุญุชูู ุนูู ุงูููุฏ ุงููุนูู ูุงูุชุทุจูู ุงูุชููู. ูุง ุชุจุฏุฃ ุงูุชูููุฐ ุญุชู ูุฑุงุฌุนุฉ ูุงุนุชูุงุฏ ุฌููุน ุฃุฌุฒุงุก ุงูุฎุทุฉ.
