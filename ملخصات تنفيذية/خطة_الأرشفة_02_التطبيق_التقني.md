# Ø®Ø·Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„Ø´Ø§Ù…Ù„ - Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ‚Ù†ÙŠ

## ğŸ“‹ Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©
Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù…Ù† Ø®Ø·Ø© ØªØ·ÙˆÙŠØ± Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„Ø´Ø§Ù…Ù„ØŒ ÙŠØ±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ‚Ù†ÙŠ Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù„Ù†Ø¸Ø§Ù…ØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ù„ÙƒÙˆØ¯ ÙˆÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….

---

## ğŸ—ï¸ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©

### ğŸ—„ï¸ Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ

#### Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
```sql
-- Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ (Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ù…ÙÙˆØµÙ‰ Ø¨Ù‡)
CREATE DATABASE kitchen_factory_archive 
CHARACTER SET utf8mb4 
COLLATE utf8mb4_unicode_ci;

-- Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø®Ø·Ø· Ù…Ù†ÙØµÙ„ (Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø¨Ø¯ÙŠÙ„)
-- CREATE SCHEMA archive;

USE kitchen_factory_archive;
```

#### Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
```sql
-- Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
CREATE TABLE archive_metadata (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    source_table VARCHAR(50) NOT NULL COMMENT 'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£ØµÙ„ÙŠ',
    source_id INTEGER NOT NULL COMMENT 'Ù…Ø¹Ø±Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ',
    archived_at DATETIME NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT 'ØªØ§Ø±ÙŠØ® Ø§Ù„Ø£Ø±Ø´ÙØ©',
    archived_by VARCHAR(100) NOT NULL COMMENT 'Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø°ÙŠ Ù‚Ø§Ù… Ø¨Ø§Ù„Ø£Ø±Ø´ÙØ©',
    archive_reason VARCHAR(500) COMMENT 'Ø³Ø¨Ø¨ Ø§Ù„Ø£Ø±Ø´ÙØ©',
    archive_type ENUM('manual', 'automatic', 'scheduled') DEFAULT 'automatic' COMMENT 'Ù†ÙˆØ¹ Ø§Ù„Ø£Ø±Ø´ÙØ©',
    original_record_json LONGTEXT COMMENT 'Ù†Ø³Ø®Ø© JSON Ù…Ù† Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ',
    can_restore BOOLEAN DEFAULT TRUE COMMENT 'Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„ØŸ',
    restore_conditions TEXT COMMENT 'Ø´Ø±ÙˆØ· Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©',
    data_size_bytes INTEGER COMMENT 'Ø­Ø¬Ù… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©',
    checksum VARCHAR(64) COMMENT 'Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø³Ù„Ø§Ù…Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª',
    expires_at DATETIME COMMENT 'ØªØ§Ø±ÙŠØ® Ø§Ù†ØªÙ‡Ø§Ø¡ ØµÙ„Ø§Ø­ÙŠØ© Ø§Ù„Ø£Ø±Ø´ÙŠÙ',
    
    -- Ø§Ù„ÙÙ‡Ø§Ø±Ø³
    INDEX idx_source_table_id (source_table, source_id),
    INDEX idx_archived_at (archived_at),
    INDEX idx_archived_by (archived_by),
    INDEX idx_archive_type (archive_type),
    INDEX idx_can_restore (can_restore),
    INDEX idx_expires_at (expires_at)
) COMMENT 'Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙØ©';

-- Ø¬Ø¯ÙˆÙ„ ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
CREATE TABLE archive_relationships (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    parent_table VARCHAR(50) NOT NULL COMMENT 'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø¨',
    parent_id INTEGER NOT NULL COMMENT 'Ù…Ø¹Ø±Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£Ø¨',
    child_table VARCHAR(50) NOT NULL COMMENT 'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„ÙØ±Ø¹ÙŠ',
    child_id INTEGER NOT NULL COMMENT 'Ù…Ø¹Ø±Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„ÙØ±Ø¹ÙŠ',
    relationship_type VARCHAR(50) COMMENT 'Ù†ÙˆØ¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©',
    archived_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    archive_batch_id INTEGER COMMENT 'Ù…Ø¹Ø±Ù Ø¯ÙØ¹Ø© Ø§Ù„Ø£Ø±Ø´ÙØ©',
    
    FOREIGN KEY (archive_batch_id) REFERENCES archive_metadata(id) ON DELETE SET NULL,
    INDEX idx_parent (parent_table, parent_id),
    INDEX idx_child (child_table, child_id),
    INDEX idx_batch (archive_batch_id),
    INDEX idx_relationship_type (relationship_type)
) COMMENT 'Ø¬Ø¯ÙˆÙ„ ØªØªØ¨Ø¹ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©';

-- Ø¬Ø¯ÙˆÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ
CREATE TABLE archive_statistics (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    table_name VARCHAR(50) NOT NULL UNIQUE COMMENT 'Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„',
    total_archived INTEGER DEFAULT 0 COMMENT 'Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©',
    total_size_mb DECIMAL(10,2) DEFAULT 0 COMMENT 'Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¨Ø§Ù„Ù…ÙŠØ¬Ø§Ø¨Ø§ÙŠØª',
    last_archive_date DATETIME COMMENT 'ØªØ§Ø±ÙŠØ® Ø¢Ø®Ø± Ø£Ø±Ø´ÙØ©',
    last_restore_date DATETIME COMMENT 'ØªØ§Ø±ÙŠØ® Ø¢Ø®Ø± Ø§Ø³ØªØ¹Ø§Ø¯Ø©',
    average_archive_age_days INTEGER COMMENT 'Ù…ØªÙˆØ³Ø· Ø¹Ù…Ø± Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©',
    archive_success_rate DECIMAL(5,2) DEFAULT 100.00 COMMENT 'Ù…Ø¹Ø¯Ù„ Ù†Ø¬Ø§Ø­ Ø§Ù„Ø£Ø±Ø´ÙØ©',
    last_updated DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_table_name (table_name),
    INDEX idx_last_archive (last_archive_date),
    INDEX idx_success_rate (archive_success_rate)
) COMMENT 'Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙØ© Ù„ÙƒÙ„ Ø¬Ø¯ÙˆÙ„';

-- Ø¬Ø¯ÙˆÙ„ Ø³Ø¬Ù„ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙØ©
CREATE TABLE archive_operations_log (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    operation_type ENUM('archive', 'restore', 'delete', 'verify', 'search') NOT NULL,
    table_name VARCHAR(50) COMMENT 'Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…ØªØ£Ø«Ø±',
    record_count INTEGER DEFAULT 0 COMMENT 'Ø¹Ø¯Ø¯ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©',
    operation_start DATETIME DEFAULT CURRENT_TIMESTAMP,
    operation_end DATETIME,
    duration_seconds INTEGER COMMENT 'Ù…Ø¯Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø¨Ø§Ù„Ø«ÙˆØ§Ù†ÙŠ',
    status ENUM('running', 'completed', 'failed', 'cancelled') DEFAULT 'running',
    error_message TEXT COMMENT 'Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„ÙØ´Ù„',
    performed_by VARCHAR(100) COMMENT 'Ù…Ù† Ù‚Ø§Ù… Ø¨Ø§Ù„Ø¹Ù…Ù„ÙŠØ©',
    batch_size INTEGER COMMENT 'Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©',
    affected_records TEXT COMMENT 'Ù‚Ø§Ø¦Ù…Ø© Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…ØªØ£Ø«Ø±Ø© (JSON)',
    performance_metrics JSON COMMENT 'Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø£Ø¯Ø§Ø¡',
    
    INDEX idx_operation_type (operation_type),
    INDEX idx_operation_start (operation_start),
    INDEX idx_status (status),
    INDEX idx_table_name (table_name),
    INDEX idx_performed_by (performed_by)
) COMMENT 'Ø³Ø¬Ù„ Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙØ© ÙˆØ§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©';

-- Ø¬Ø¯ÙˆÙ„ Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©
CREATE TABLE archive_scheduler (
    id INTEGER PRIMARY KEY AUTO_INCREMENT,
    table_name VARCHAR(50) NOT NULL COMMENT 'Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø±Ø§Ø¯ Ø£Ø±Ø´ÙØªÙ‡',
    schedule_name VARCHAR(100) NOT NULL COMMENT 'Ø§Ø³Ù… Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø©',
    is_enabled BOOLEAN DEFAULT TRUE COMMENT 'Ù‡Ù„ Ø§Ù„Ø¬Ø¯ÙˆÙ„Ø© Ù…ÙØ¹Ù„Ø©ØŸ',
    cron_expression VARCHAR(100) COMMENT 'ØªØ¹Ø¨ÙŠØ± Ø§Ù„ÙˆÙ‚Øª (Ù…Ø«Ù„ cron)',
    archive_condition TEXT NOT NULL COMMENT 'Ø´Ø±Ø· SQL Ù„Ù„Ø£Ø±Ø´ÙØ©',
    batch_size INTEGER DEFAULT 100 COMMENT 'Ø­Ø¬Ù… Ø§Ù„Ø¯ÙØ¹Ø©',
    max_records_per_run INTEGER DEFAULT 1000 COMMENT 'Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø³Ø¬Ù„Ø§Øª ÙÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø­Ø¯',
    last_run DATETIME COMMENT 'Ø¢Ø®Ø± ØªØ´ØºÙŠÙ„',
    next_run DATETIME COMMENT 'Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠ',
    success_count INTEGER DEFAULT 0 COMMENT 'Ø¹Ø¯Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©',
    failure_count INTEGER DEFAULT 0 COMMENT 'Ø¹Ø¯Ø¯ Ø§Ù„ØªØ´ØºÙŠÙ„Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©',
    last_error TEXT COMMENT 'Ø¢Ø®Ø± Ø±Ø³Ø§Ù„Ø© Ø®Ø·Ø£',
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    created_by VARCHAR(100),
    
    INDEX idx_table_name (table_name),
    INDEX idx_enabled (is_enabled),
    INDEX idx_next_run (next_run),
    INDEX idx_schedule_name (schedule_name)
) COMMENT 'Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„Ø©';
```

#### Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¥Ù†Ø´Ø§Ø¡ Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
```sql
-- Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø·Ù„Ø¨ÙŠØ§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
CREATE TABLE archive_orders LIKE kitchen_factory.orders;
ALTER TABLE archive_orders ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_orders ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;
ALTER TABLE archive_orders ADD INDEX idx_archive_metadata (archive_metadata_id);
ALTER TABLE archive_orders ADD INDEX idx_archived_at (archived_at);

CREATE TABLE archive_stages LIKE kitchen_factory.stages;
ALTER TABLE archive_stages ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_stages ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_order_costs LIKE kitchen_factory.order_costs;
ALTER TABLE archive_order_costs ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_order_costs ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_order_material LIKE kitchen_factory.order_material;
ALTER TABLE archive_order_material ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_order_material ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_documents LIKE kitchen_factory.documents;
ALTER TABLE archive_documents ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_documents ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_received_order LIKE kitchen_factory.received_order;
ALTER TABLE archive_received_order ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_received_order ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_payments LIKE kitchen_factory.payments;
ALTER TABLE archive_payments ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_payments ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

-- Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
CREATE TABLE archive_purchase_invoice LIKE kitchen_factory.purchase_invoice;
ALTER TABLE archive_purchase_invoice ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_purchase_invoice ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_purchase_invoice_item LIKE kitchen_factory.purchase_invoice_item;
ALTER TABLE archive_purchase_invoice_item ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_purchase_invoice_item ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_supplier_payment LIKE kitchen_factory.supplier_payment;
ALTER TABLE archive_supplier_payment ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_supplier_payment ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

-- Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„ÙÙ†ÙŠÙŠÙ† Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
CREATE TABLE archive_technician_due LIKE kitchen_factory.technician_due;
ALTER TABLE archive_technician_due ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_technician_due ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_technician_payment LIKE kitchen_factory.technician_payment;
ALTER TABLE archive_technician_payment ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_technician_payment ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

-- Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
CREATE TABLE archive_audit_log LIKE kitchen_factory.audit_log;
ALTER TABLE archive_audit_log ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_audit_log ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;

CREATE TABLE archive_material_consumption LIKE kitchen_factory.material_consumption;
ALTER TABLE archive_material_consumption ADD COLUMN archive_metadata_id INTEGER;
ALTER TABLE archive_material_consumption ADD COLUMN archived_at DATETIME DEFAULT CURRENT_TIMESTAMP;
```

---

## ğŸ”§ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: ØªØ·ÙˆÙŠØ± Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ

### ğŸ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„Ø¯ÙˆØ§Ù„ ÙÙŠ Flask

#### Ø§Ù„Ø®Ø·ÙˆØ© 1: Ø¥Ø¶Ø§ÙØ© Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø£Ø±Ø´ÙØ© Ø¥Ù„Ù‰ app.py
```python
# Ø¥Ø¶Ø§ÙØ© Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ ÙÙŠ app.py Ø¨Ø¹Ø¯ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©

# ==================== Ù†Ù…Ø§Ø°Ø¬ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ© ====================

class ArchiveMetadata(db.Model):
    """Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ© Ù„Ù„Ø£Ø±Ø´ÙØ©"""
    __tablename__ = 'archive_metadata'
    __bind_key__ = 'archive'  # Ø±Ø¨Ø· Ø¨Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ
    
    id = db.Column(db.Integer, primary_key=True)
    source_table = db.Column(db.String(50), nullable=False)
    source_id = db.Column(db.Integer, nullable=False)
    archived_at = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    archived_by = db.Column(db.String(100), nullable=False)
    archive_reason = db.Column(db.String(500))
    archive_type = db.Column(db.Enum('manual', 'automatic', 'scheduled'), default='automatic')
    original_record_json = db.Column(db.Text)
    can_restore = db.Column(db.Boolean, default=True)
    restore_conditions = db.Column(db.Text)
    data_size_bytes = db.Column(db.Integer)
    checksum = db.Column(db.String(64))
    expires_at = db.Column(db.DateTime)


class ArchiveRelationship(db.Model):
    """Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©"""
    __tablename__ = 'archive_relationships'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    parent_table = db.Column(db.String(50), nullable=False)
    parent_id = db.Column(db.Integer, nullable=False)
    child_table = db.Column(db.String(50), nullable=False)
    child_id = db.Column(db.Integer, nullable=False)
    relationship_type = db.Column(db.String(50))
    archived_at = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    archive_batch_id = db.Column(db.Integer, db.ForeignKey('archive_metadata.id'))


class ArchiveStatistics(db.Model):
    """Ù†Ù…ÙˆØ°Ø¬ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙØ©"""
    __tablename__ = 'archive_statistics'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    table_name = db.Column(db.String(50), nullable=False, unique=True)
    total_archived = db.Column(db.Integer, default=0)
    total_size_mb = db.Column(db.Numeric(10, 2), default=0)
    last_archive_date = db.Column(db.DateTime)
    last_restore_date = db.Column(db.DateTime)
    average_archive_age_days = db.Column(db.Integer)
    archive_success_rate = db.Column(db.Numeric(5, 2), default=100.00)
    last_updated = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))


class ArchiveOperationsLog(db.Model):
    """Ù†Ù…ÙˆØ°Ø¬ Ø³Ø¬Ù„ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙØ©"""
    __tablename__ = 'archive_operations_log'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    operation_type = db.Column(db.Enum('archive', 'restore', 'delete', 'verify', 'search'), nullable=False)
    table_name = db.Column(db.String(50))
    record_count = db.Column(db.Integer, default=0)
    operation_start = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    operation_end = db.Column(db.DateTime)
    duration_seconds = db.Column(db.Integer)
    status = db.Column(db.Enum('running', 'completed', 'failed', 'cancelled'), default='running')
    error_message = db.Column(db.Text)
    performed_by = db.Column(db.String(100))
    batch_size = db.Column(db.Integer)
    affected_records = db.Column(db.Text)  # JSON string
    performance_metrics = db.Column(db.JSON)


class ArchiveScheduler(db.Model):
    """Ù†Ù…ÙˆØ°Ø¬ Ø¬Ø¯ÙˆÙ„Ø© Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©"""
    __tablename__ = 'archive_scheduler'
    __bind_key__ = 'archive'
    
    id = db.Column(db.Integer, primary_key=True)
    table_name = db.Column(db.String(50), nullable=False)
    schedule_name = db.Column(db.String(100), nullable=False)
    is_enabled = db.Column(db.Boolean, default=True)
    cron_expression = db.Column(db.String(100))
    archive_condition = db.Column(db.Text, nullable=False)
    batch_size = db.Column(db.Integer, default=100)
    max_records_per_run = db.Column(db.Integer, default=1000)
    last_run = db.Column(db.DateTime)
    next_run = db.Column(db.DateTime)
    success_count = db.Column(db.Integer, default=0)
    failure_count = db.Column(db.Integer, default=0)
    last_error = db.Column(db.Text)
    created_at = db.Column(db.DateTime, default=lambda: datetime.now(timezone.utc))
    created_by = db.Column(db.String(100))
```

#### Ø§Ù„Ø®Ø·ÙˆØ© 2: Ø¥Ø¶Ø§ÙØ© Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
```python
# ==================== Ø¯ÙˆØ§Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ© ====================

import hashlib
import json
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
import logging

# Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø®ØµØµ Ù„Ù„Ø£Ø±Ø´ÙØ©
archive_logger = logging.getLogger('archive')

def get_archive_setting(key: str, default: Any = None, value_type: str = 'string') -> Any:
    """Ø¬Ù„Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ©"""
    return get_system_setting(key, default, value_type)

def calculate_data_checksum(data: dict) -> str:
    """Ø­Ø³Ø§Ø¨ checksum Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø³Ù„Ø§Ù…Ø©"""
    json_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(json_str.encode('utf-8')).hexdigest()

def log_archive_operation(operation_type: str, table_name: str = None, 
                         record_count: int = 0, performed_by: str = None,
                         batch_size: int = None) -> ArchiveOperationsLog:
    """ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø£Ø±Ø´ÙØ© ÙÙŠ Ø§Ù„Ø³Ø¬Ù„"""
    operation = ArchiveOperationsLog(
        operation_type=operation_type,
        table_name=table_name,
        record_count=record_count,
        performed_by=performed_by or (current_user.username if current_user.is_authenticated else 'system'),
        batch_size=batch_size,
        status='running'
    )
    
    db.session.add(operation)
    db.session.commit()
    return operation

def complete_archive_operation(operation: ArchiveOperationsLog, 
                             status: str = 'completed',
                             error_message: str = None,
                             affected_records: List = None,
                             performance_metrics: Dict = None):
    """Ø¥ÙƒÙ…Ø§Ù„ Ø¹Ù…Ù„ÙŠØ© Ø£Ø±Ø´ÙØ© ÙˆØªØ­Ø¯ÙŠØ« Ø§Ù„Ø³Ø¬Ù„"""
    operation.operation_end = datetime.now(timezone.utc)
    operation.duration_seconds = int((operation.operation_end - operation.operation_start).total_seconds())
    operation.status = status
    operation.error_message = error_message
    
    if affected_records:
        operation.affected_records = json.dumps(affected_records)
    
    if performance_metrics:
        operation.performance_metrics = performance_metrics
    
    db.session.commit()

def update_archive_statistics(table_name: str, archived_count: int = 0, 
                            size_mb: float = 0, operation_type: str = 'archive'):
    """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙØ©"""
    stats = ArchiveStatistics.query.filter_by(table_name=table_name).first()
    
    if not stats:
        stats = ArchiveStatistics(
            table_name=table_name,
            total_archived=0,
            total_size_mb=0,
            archive_success_rate=100.00
        )
        db.session.add(stats)
    
    if operation_type == 'archive':
        stats.total_archived += archived_count
        stats.total_size_mb += size_mb
        stats.last_archive_date = datetime.now(timezone.utc)
    elif operation_type == 'restore':
        stats.total_archived = max(0, stats.total_archived - archived_count)
        stats.last_restore_date = datetime.now(timezone.utc)
    
    stats.last_updated = datetime.now(timezone.utc)
    db.session.commit()

def create_archive_metadata(source_table: str, source_id: int, 
                          original_record: dict, archive_reason: str,
                          archive_type: str = 'automatic') -> ArchiveMetadata:
    """Ø¥Ù†Ø´Ø§Ø¡ Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØµÙÙŠØ© Ù„Ù„Ø£Ø±Ø´ÙØ©"""
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ JSON ÙˆØ­Ø³Ø§Ø¨ Ø§Ù„checksum
    json_data = json.dumps(original_record, default=str, ensure_ascii=False)
    checksum = calculate_data_checksum(original_record)
    data_size = len(json_data.encode('utf-8'))
    
    # ØªØ­Ø¯ÙŠØ¯ ØªØ§Ø±ÙŠØ® Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
    retention_years = get_archive_setting('archive_retention_years', 7, 'integer')
    expires_at = datetime.now(timezone.utc).replace(year=datetime.now().year + retention_years)
    
    metadata = ArchiveMetadata(
        source_table=source_table,
        source_id=source_id,
        archived_by=current_user.username if current_user.is_authenticated else 'system',
        archive_reason=archive_reason,
        archive_type=archive_type,
        original_record_json=json_data,
        data_size_bytes=data_size,
        checksum=checksum,
        expires_at=expires_at
    )
    
    db.session.add(metadata)
    db.session.flush()  # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±Ù
    return metadata

def archive_single_record(source_table: str, record_id: int, 
                        archive_reason: str = 'ØªÙ„Ù‚Ø§Ø¦ÙŠØ©',
                        archive_type: str = 'automatic') -> bool:
    """Ø£Ø±Ø´ÙØ© Ø³Ø¬Ù„ ÙˆØ§Ø­Ø¯ Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©"""
    
    try:
        archive_logger.info(f"Ø¨Ø¯Ø¡ Ø£Ø±Ø´ÙØ© Ø³Ø¬Ù„ {record_id} Ù…Ù† Ø¬Ø¯ÙˆÙ„ {source_table}")
        
        # Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø£Ø±Ø´ÙØ© ÙÙŠ Ø§Ù„Ø³Ø¬Ù„
        operation = log_archive_operation('archive', source_table, 1)
        
        # Ø¬Ù„Ø¨ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ
        original_record = get_record_by_table_and_id(source_table, record_id)
        if not original_record:
            raise Exception(f"Ø§Ù„Ø³Ø¬Ù„ {record_id} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ø¬Ø¯ÙˆÙ„ {source_table}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
        metadata = create_archive_metadata(
            source_table, record_id, original_record, archive_reason, archive_type
        )
        
        # Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        archive_main_record(source_table, original_record, metadata.id)
        
        # Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
        related_count = archive_related_records(source_table, record_id, metadata.id)
        
        # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£ØµÙ„ÙŠØ©
        delete_original_records(source_table, record_id)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        update_archive_statistics(source_table, 1, metadata.data_size_bytes / (1024 * 1024))
        
        # Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        complete_archive_operation(
            operation, 
            'completed',
            affected_records=[record_id],
            performance_metrics={
                'main_record': 1,
                'related_records': related_count,
                'data_size_mb': metadata.data_size_bytes / (1024 * 1024)
            }
        )
        
        archive_logger.info(f"ØªÙ… Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„ {record_id} Ø¨Ù†Ø¬Ø§Ø­")
        return True
        
    except Exception as e:
        archive_logger.error(f"ÙØ´Ù„ Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„ {record_id}: {str(e)}")
        complete_archive_operation(operation, 'failed', str(e))
        db.session.rollback()
        return False

def get_record_by_table_and_id(table_name: str, record_id: int) -> dict:
    """Ø¬Ù„Ø¨ Ø³Ø¬Ù„ Ù…Ù† Ø¬Ø¯ÙˆÙ„ Ù…Ø­Ø¯Ø¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±Ù"""
    
    # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ ÙˆØ§Ù„Ù†Ù…Ø§Ø°Ø¬
    table_models = {
        'orders': Order,
        'purchase_invoice': PurchaseInvoice,
        'technician_due': TechnicianDue,
        'audit_log': AuditLog,
        'material_consumption': MaterialConsumption
    }
    
    model_class = table_models.get(table_name)
    if not model_class:
        raise Exception(f"Ø¬Ø¯ÙˆÙ„ ØºÙŠØ± Ù…Ø¯Ø¹ÙˆÙ…: {table_name}")
    
    record = model_class.query.get(record_id)
    if not record:
        return None
    
    # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¬Ù„ Ø¥Ù„Ù‰ dictionary
    return {column.name: getattr(record, column.name) 
            for column in model_class.__table__.columns}

def archive_main_record(source_table: str, record_data: dict, metadata_id: int):
    """Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    # Ø®Ø±ÙŠØ·Ø© Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£Ø±Ø´ÙØ©
    archive_tables = {
        'orders': 'archive_orders',
        'purchase_invoice': 'archive_purchase_invoice', 
        'technician_due': 'archive_technician_due',
        'audit_log': 'archive_audit_log',
        'material_consumption': 'archive_material_consumption'
    }
    
    archive_table = archive_tables.get(source_table)
    if not archive_table:
        raise Exception(f"Ø¬Ø¯ÙˆÙ„ Ø£Ø±Ø´ÙŠÙ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ Ù„Ù€: {source_table}")
    
    # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙØ©
    record_data['archive_metadata_id'] = metadata_id
    record_data['archived_at'] = datetime.now(timezone.utc)
    
    # Ø¥Ø¯Ø±Ø§Ø¬ Ø§Ù„Ø³Ø¬Ù„ ÙÙŠ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø±Ø´ÙŠÙ
    columns = ', '.join(record_data.keys())
    placeholders = ', '.join(['%s'] * len(record_data))
    
    query = f"INSERT INTO {archive_table} ({columns}) VALUES ({placeholders})"
    
    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ Ù‚Ø§Ø¹Ø¯Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ
    archive_db = db.get_engine(app, 'archive')
    archive_db.execute(query, list(record_data.values()))

def archive_related_records(source_table: str, parent_id: int, metadata_id: int) -> int:
    """Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª Ù„Ù„Ø¬Ø¯Ø§ÙˆÙ„
    relationships_config = {
        'orders': [
            {'table': 'stages', 'foreign_key': 'order_id', 'archive_table': 'archive_stages'},
            {'table': 'order_costs', 'foreign_key': 'order_id', 'archive_table': 'archive_order_costs'},
            {'table': 'order_material', 'foreign_key': 'order_id', 'archive_table': 'archive_order_material'},
            {'table': 'payments', 'foreign_key': 'order_id', 'archive_table': 'archive_payments'},
            {'table': 'documents', 'foreign_key': 'order_id', 'archive_table': 'archive_documents'},
            {'table': 'received_order', 'foreign_key': 'order_id', 'archive_table': 'archive_received_order'}
        ],
        'purchase_invoice': [
            {'table': 'purchase_invoice_item', 'foreign_key': 'invoice_id', 'archive_table': 'archive_purchase_invoice_item'},
            {'table': 'supplier_payment', 'foreign_key': 'invoice_id', 'archive_table': 'archive_supplier_payment'}
        ]
    }
    
    relationships = relationships_config.get(source_table, [])
    total_archived = 0
    
    for rel in relationships:
        related_records = db.session.execute(
            f"SELECT * FROM {rel['table']} WHERE {rel['foreign_key']} = %s",
            [parent_id]
        ).fetchall()
        
        for record in related_records:
            # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø³Ø¬Ù„ Ø¥Ù„Ù‰ dictionary
            record_dict = dict(record)
            record_dict['archive_metadata_id'] = metadata_id
            record_dict['archived_at'] = datetime.now(timezone.utc)
            
            # Ø¥Ø¯Ø±Ø§Ø¬ ÙÙŠ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø£Ø±Ø´ÙŠÙ
            columns = ', '.join(record_dict.keys())
            placeholders = ', '.join(['%s'] * len(record_dict))
            
            query = f"INSERT INTO {rel['archive_table']} ({columns}) VALUES ({placeholders})"
            archive_db = db.get_engine(app, 'archive')
            archive_db.execute(query, list(record_dict.values()))
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø©
            relationship = ArchiveRelationship(
                parent_table=source_table,
                parent_id=parent_id,
                child_table=rel['table'],
                child_id=record['id'],
                relationship_type='one_to_many',
                archive_batch_id=metadata_id
            )
            db.session.add(relationship)
            
            total_archived += 1
    
    return total_archived

def delete_original_records(source_table: str, parent_id: int):
    """Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ù† Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¨Ø¹Ø¯ Ø§Ù„Ø£Ø±Ø´ÙØ©"""
    
    # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø£ÙˆÙ„Ø§Ù‹ (Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ù„Ø§Ù‚Ø§Øª)
    relationships_config = {
        'orders': ['payments', 'documents', 'received_order', 'order_material', 'order_costs', 'stages'],
        'purchase_invoice': ['supplier_payment', 'purchase_invoice_item'],
        'technician_due': []  # Ø³Ø¬Ù„ Ù…Ù†ÙØ±Ø¯
    }
    
    related_tables = relationships_config.get(source_table, [])
    foreign_key_map = {
        'orders': 'order_id',
        'purchase_invoice': 'invoice_id',
        'technician_due': 'due_id'
    }
    
    foreign_key = foreign_key_map.get(source_table, f"{source_table}_id")
    
    # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
    for table in related_tables:
        db.session.execute(f"DELETE FROM {table} WHERE {foreign_key} = %s", [parent_id])
    
    # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    db.session.execute(f"DELETE FROM {source_table} WHERE id = %s", [parent_id])
    db.session.commit()
```

#### Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø¥Ø¶Ø§ÙØ© Ø¯ÙˆØ§Ù„ Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©
```python
def archive_batch_records(source_table: str, record_ids: List[int], 
                         archive_reason: str = 'Ø¯ÙØ¹ÙŠØ©', 
                         batch_size: int = None) -> Dict[str, Any]:
    """Ø£Ø±Ø´ÙØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø³Ø¬Ù„Ø§Øª ÙÙŠ Ø¯ÙØ¹Ø§Øª"""
    
    if not batch_size:
        batch_size = get_archive_setting('archive_batch_size', 50, 'integer')
    
    total_records = len(record_ids)
    successful_count = 0
    failed_count = 0
    failed_records = []
    
    # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
    operation = log_archive_operation('archive', source_table, total_records)
    
    try:
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø³Ø¬Ù„Ø§Øª ÙÙŠ Ø¯ÙØ¹Ø§Øª
        for i in range(0, total_records, batch_size):
            batch = record_ids[i:i + batch_size]
            
            for record_id in batch:
                if archive_single_record(source_table, record_id, archive_reason, 'manual'):
                    successful_count += 1
                else:
                    failed_count += 1
                    failed_records.append(record_id)
            
            # ÙØ§ØµÙ„ Ø²Ù…Ù†ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø¯ÙØ¹Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ù…Ù„ Ø§Ù„Ø²Ø§Ø¦Ø¯
            import time
            time.sleep(0.1)
        
        # Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        status = 'completed' if failed_count == 0 else 'partial_failure'
        complete_archive_operation(
            operation,
            status,
            f"ÙØ´Ù„ ÙÙŠ {failed_count} Ù…Ù† Ø£ØµÙ„ {total_records}" if failed_count > 0 else None,
            record_ids,
            {
                'successful_count': successful_count,
                'failed_count': failed_count,
                'failed_records': failed_records,
                'batch_size': batch_size
            }
        )
        
        return {
            'success': True,
            'total_records': total_records,
            'successful_count': successful_count,
            'failed_count': failed_count,
            'failed_records': failed_records
        }
        
    except Exception as e:
        complete_archive_operation(operation, 'failed', str(e))
        return {
            'success': False,
            'error': str(e),
            'successful_count': successful_count,
            'failed_count': failed_count
        }

def auto_archive_eligible_records():
    """Ø£Ø±Ø´ÙØ© ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù„Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø¤Ù‡Ù„Ø© Ø­Ø³Ø¨ Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª"""
    
    if not get_archive_setting('archive_system_enabled', True, 'boolean'):
        return {'message': 'Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ© Ù…Ø¹Ø·Ù„'}
    
    if not get_archive_setting('archive_auto_mode_enabled', True, 'boolean'):
        return {'message': 'Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© Ù…Ø¹Ø·Ù„Ø©'}
    
    results = {}
    
    # Ø£Ø±Ø´ÙØ© Ø§Ù„Ø·Ù„Ø¨ÙŠØ§Øª Ø§Ù„Ù…Ø¤Ù‡Ù„Ø©
    order_days = get_archive_setting('order_auto_archive_days', 90, 'integer')
    eligible_orders = db.session.execute(f"""
        SELECT id FROM orders 
        WHERE status IN ('Ù…Ø³Ù„Ù‘Ù…', 'Ù…ÙƒØªÙ…Ù„')
        AND delivery_date IS NOT NULL
        AND delivery_date <= DATE_SUB(NOW(), INTERVAL {order_days} DAY)
        AND (
            SELECT COUNT(*) FROM payments p 
            WHERE p.order_id = orders.id AND p.amount > 0
        ) > 0
        AND NOT EXISTS (
            SELECT 1 FROM archive_metadata am 
            WHERE am.source_table = 'orders' AND am.source_id = orders.id
        )
        LIMIT {get_archive_setting('archive_max_daily_records', 500, 'integer')}
    """).fetchall()
    
    if eligible_orders:
        order_ids = [row[0] for row in eligible_orders]
        results['orders'] = archive_batch_records('orders', order_ids, 'ØªÙ„Ù‚Ø§Ø¦ÙŠØ© - Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„Ù…Ø¯Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©')
    
    # Ø£Ø±Ø´ÙØ© Ø§Ù„ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ù…Ø¤Ù‡Ù„Ø©
    invoice_days = get_archive_setting('invoice_auto_archive_days', 120, 'integer')
    eligible_invoices = db.session.execute(f"""
        SELECT id FROM purchase_invoice 
        WHERE status = 'paid'
        AND payment_date IS NOT NULL
        AND payment_date <= DATE_SUB(NOW(), INTERVAL {invoice_days} DAY)
        AND NOT EXISTS (
            SELECT 1 FROM archive_metadata am 
            WHERE am.source_table = 'purchase_invoice' AND am.source_id = purchase_invoice.id
        )
        LIMIT 100
    """).fetchall()
    
    if eligible_invoices:
        invoice_ids = [row[0] for row in eligible_invoices]
        results['invoices'] = archive_batch_records('purchase_invoice', invoice_ids, 'ØªÙ„Ù‚Ø§Ø¦ÙŠØ© - ÙØ§ØªÙˆØ±Ø© Ù…Ø¯ÙÙˆØ¹Ø©')
    
    # Ø£Ø±Ø´ÙØ© Ù…Ø³ØªØ­Ù‚Ø§Øª Ø§Ù„ÙÙ†ÙŠÙŠÙ† Ø§Ù„Ù…Ø¤Ù‡Ù„Ø©
    technician_days = get_archive_setting('technician_payment_archive_days', 180, 'integer')
    eligible_dues = db.session.execute(f"""
        SELECT id FROM technician_due 
        WHERE is_paid = true
        AND paid_at IS NOT NULL
        AND paid_at <= DATE_SUB(NOW(), INTERVAL {technician_days} DAY)
        AND NOT EXISTS (
            SELECT 1 FROM archive_metadata am 
            WHERE am.source_table = 'technician_due' AND am.source_id = technician_due.id
        )
        LIMIT 200
    """).fetchall()
    
    if eligible_dues:
        due_ids = [row[0] for row in eligible_dues]
        results['technician_dues'] = archive_batch_records('technician_due', due_ids, 'ØªÙ„Ù‚Ø§Ø¦ÙŠØ© - Ù…Ø³ØªØ­Ù‚ Ù…Ø¯ÙÙˆØ¹')
    
    return results

def restore_archived_record(source_table: str, record_id: int, 
                          restore_reason: str = 'Ø·Ù„Ø¨ Ø§Ø³ØªØ¹Ø§Ø¯Ø©') -> bool:
    """Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø³Ø¬Ù„ Ù…Ù† Ø§Ù„Ø£Ø±Ø´ÙŠÙ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    try:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙˆØµÙÙŠØ©
        metadata = ArchiveMetadata.query.filter_by(
            source_table=source_table, 
            source_id=record_id
        ).first()
        
        if not metadata:
            raise Exception(f"Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø³Ø¬Ù„ Ù…Ø¤Ø±Ø´Ù Ù„Ù„Ù…Ø¹Ø±Ù {record_id} ÙÙŠ Ø¬Ø¯ÙˆÙ„ {source_table}")
        
        if not metadata.can_restore:
            raise Exception(f"Ø§Ù„Ø³Ø¬Ù„ {record_id} Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¹Ø§Ø¯ØªÙ‡: {metadata.restore_conditions}")
        
        # ØªØ³Ø¬ÙŠÙ„ Ø¨Ø¯Ø§ÙŠØ© Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©
        operation = log_archive_operation('restore', source_table, 1)
        
        # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        restore_main_record(source_table, record_id, metadata.id)
        
        # Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©
        restored_count = restore_related_records(source_table, record_id, metadata.id)
        
        # Ø­Ø°Ù Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ù† Ø§Ù„Ø£Ø±Ø´ÙŠÙ
        delete_archived_records(source_table, record_id, metadata.id)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        update_archive_statistics(source_table, -1, operation_type='restore')
        
        # Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
        complete_archive_operation(
            operation,
            'completed',
            affected_records=[record_id],
            performance_metrics={
                'main_record': 1,
                'related_records': restored_count,
                'restore_reason': restore_reason
            }
        )
        
        archive_logger.info(f"ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„ {record_id} Ù…Ù† {source_table} Ø¨Ù†Ø¬Ø§Ø­")
        return True
        
    except Exception as e:
        archive_logger.error(f"ÙØ´Ù„ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„ {record_id}: {str(e)}")
        complete_archive_operation(operation, 'failed', str(e))
        return False

def search_archived_records(search_params: Dict[str, Any]) -> List[Dict]:
    """Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©"""
    
    table_name = search_params.get('table_name')
    start_date = search_params.get('start_date')
    end_date = search_params.get('end_date')
    archive_reason = search_params.get('archive_reason')
    archived_by = search_params.get('archived_by')
    limit = search_params.get('limit', get_archive_setting('archive_search_results_limit', 100, 'integer'))
    
    # ØªØ³Ø¬ÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø­Ø«
    operation = log_archive_operation('search', table_name)
    
    try:
        # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«
        query = "SELECT * FROM archive_metadata WHERE 1=1"
        params = []
        
        if table_name:
            query += " AND source_table = %s"
            params.append(table_name)
        
        if start_date:
            query += " AND archived_at >= %s"
            params.append(start_date)
        
        if end_date:
            query += " AND archived_at <= %s"
            params.append(end_date)
        
        if archive_reason:
            query += " AND archive_reason LIKE %s"
            params.append(f"%{archive_reason}%")
        
        if archived_by:
            query += " AND archived_by = %s"
            params.append(archived_by)
        
        query += f" ORDER BY archived_at DESC LIMIT {limit}"
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¨Ø­Ø«
        archive_db = db.get_engine(app, 'archive')
        results = archive_db.execute(query, params).fetchall()
        
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³
        search_results = []
        for row in results:
            result = dict(row)
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…ØªØ§Ø­Ø©
            if result['original_record_json']:
                result['original_data'] = json.loads(result['original_record_json'])
            search_results.append(result)
        
        # Ø¥ÙƒÙ…Ø§Ù„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø¨Ø­Ø«
        complete_archive_operation(
            operation,
            'completed',
            performance_metrics={
                'results_count': len(search_results),
                'search_params': search_params
            }
        )
        
        return search_results
        
    except Exception as e:
        complete_archive_operation(operation, 'failed', str(e))
        raise e
```

---

## ğŸŒ Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©: ØªØ·ÙˆÙŠØ± Ø§Ù„ÙˆØ§Ø¬Ù‡Ø§Øª

### ğŸ“± Ø¥Ø¶Ø§ÙØ© Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙØ© Ø¥Ù„Ù‰ Flask

#### Ø§Ù„Ø®Ø·ÙˆØ© 1: Ù…Ø³Ø§Ø±Ø§Øª Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø£Ø±Ø´ÙØ©
```python
# ==================== Ù…Ø³Ø§Ø±Ø§Øª Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ© ====================

# Ø¯ÙŠÙƒÙˆØ±ÙŠØªØ± Ù„Ù„ØªØ­ÙƒÙ… Ø¨ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ
def require_archive_permission(permission_level='viewer'):
    """Ø¯ÙŠÙƒÙˆØ±ÙŠØªØ± Ù„Ù„ØªØ­ÙƒÙ… Ø¨ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„ÙˆØµÙˆÙ„ Ù„Ù„Ø£Ø±Ø´ÙŠÙ"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            if not current_user.is_authenticated:
                flash('ÙŠØ¬Ø¨ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø£ÙˆÙ„Ø§Ù‹', 'danger')
                return redirect(url_for('login'))
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¯ÙˆØ§Ø± Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø© Ù„ÙƒÙ„ Ù…Ø³ØªÙˆÙ‰ ØµÙ„Ø§Ø­ÙŠØ©
            permission_roles = {
                'viewer': ['Ù…Ø¯ÙŠØ±', 'Ù…Ø³Ø¤ÙˆÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª'],
                'manager': ['Ù…Ø¯ÙŠØ±'],
                'admin': ['Ù…Ø¯ÙŠØ±']
            }
            
            allowed_roles = permission_roles.get(permission_level, ['Ù…Ø¯ÙŠØ±'])
            
            if current_user.role not in allowed_roles:
                flash('Ù„ÙŠØ³ Ù„Ø¯ÙŠÙƒ ØµÙ„Ø§Ø­ÙŠØ© Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙŠÙ', 'danger')
                return redirect(url_for('dashboard'))
            
            return func(*args, **kwargs)
        return wrapper
    return decorator

@app.route('/archive')
@login_required
@require_archive_permission('viewer')
def archive_dashboard():
    """Ù„ÙˆØ­Ø© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø±Ø´ÙŠÙ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    
    # Ø¬Ù…Ø¹ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
    stats = {}
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©
    archive_stats = ArchiveStatistics.query.all()
    stats['tables'] = {stat.table_name: {
        'total_archived': stat.total_archived,
        'total_size_mb': float(stat.total_size_mb or 0),
        'last_archive_date': stat.last_archive_date,
        'success_rate': float(stat.archive_success_rate or 100)
    } for stat in archive_stats}
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©
    recent_operations = ArchiveOperationsLog.query.order_by(
        ArchiveOperationsLog.operation_start.desc()
    ).limit(10).all()
    
    stats['recent_operations'] = recent_operations
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ§Øª
    stats['totals'] = {
        'total_archived_records': sum([stat.total_archived for stat in archive_stats]),
        'total_size_mb': sum([float(stat.total_size_mb or 0) for stat in archive_stats]),
        'active_schedules': ArchiveScheduler.query.filter_by(is_enabled=True).count()
    }
    
    return render_template('archive/dashboard.html', stats=stats)

@app.route('/archive/orders')
@login_required  
@require_archive_permission('viewer')
def archived_orders():
    """Ø¹Ø±Ø¶ Ø§Ù„Ø·Ù„Ø¨ÙŠØ§Øª Ø§Ù„Ù…Ø¤Ø±Ø´ÙØ©"""
    
    page = request.args.get('page', 1, type=int)
    per_page = 20
    
    # ÙÙ„Ø§ØªØ± Ø§Ù„Ø¨Ø­Ø«
    customer_name = request.args.get('customer_name', '')
    start_date = request.args.get('start_date', '')
    end_date = request.args.get('end_date', '')
    
    # Ø¨Ù†Ø§Ø¡ Ø§Ø³ØªØ¹Ù„Ø§Ù… Ø§Ù„Ø¨Ø­Ø«
    query = """
        SELECT 
            am.*,
            JSON_EXTRACT(am.original_record_json, '$.customer_name') as customer_name,
            JSON_EXTRACT(am.original_record_json, '$.total_value') as total_value,
            JSON_EXTRACT(am.original_record_json, '$.order_date') as order_date,
            JSON_EXTRACT(am.original_record_json, '$.delivery_date') as delivery_date
        FROM archive_metadata am 
        WHERE am.source_table = 'orders'
    """
    
    params = []
    
    if customer_name:
        query += " AND JSON_EXTRACT(am.original_record_json, '$.customer_name') LIKE %s"
        params.append(f"%{customer_name}%")
    
    if start_date:
        query += " AND am.archived_at >= %s"
        params.append(start_date)
    
    if end_date:
        query += " AND am.archived_at <= %s"
        params.append(end_date)
    
    query += " ORDER BY am.archived_at DESC"
    
    # ØªÙ†ÙÙŠØ° Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… Ù…Ø¹ Ø§Ù„ØªØµÙØ­
    offset = (page - 1) * per_page
    paginated_query = f"{query} LIMIT {per_page} OFFSET {offset}"
    
    archive_db = db.get_engine(app, 'archive')
    results = archive_db.execute(paginated_query, params).fetchall()
    
    # Ø­Ø³Ø§Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù„Ù„ØªØµÙØ­
    count_query = f"SELECT COUNT(*) FROM ({query}) as count_subquery"
    total = archive_db.execute(count_query, params).scalar()
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù† Ø§Ù„ØªØµÙØ­
    pagination = {
        'page': page,
        'per_page': per_page,
        'total': total,
        'pages': (total + per_page - 1) // per_page,
        'has_prev': page > 1,
        'has_next': page * per_page < total,
        'prev_num': page - 1 if page > 1 else None,
        'next_num': page + 1 if page * per_page < total else None
    }
    
    return render_template('archive/orders.html', 
                         results=results, 
                         pagination=pagination,
                         filters={'customer_name': customer_name, 
                                'start_date': start_date, 
                                'end_date': end_date})

@app.route('/archive/search', methods=['GET', 'POST'])
@login_required
@require_archive_permission('viewer')
def archive_search():
    """Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙÙŠ Ø§Ù„Ø£Ø±Ø´ÙŠÙ"""
    
    results = []
    search_params = {}
    
    if request.method == 'POST':
        search_params = {
            'table_name': request.form.get('table_name'),
            'start_date': request.form.get('start_date'),
            'end_date': request.form.get('end_date'),
            'archive_reason': request.form.get('archive_reason'),
            'archived_by': request.form.get('archived_by'),
            'limit': int(request.form.get('limit', 100))
        }
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„ÙØ§Ø±ØºØ©
        search_params = {k: v for k, v in search_params.items() if v}
        
        try:
            results = search_archived_records(search_params)
            flash(f'ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(results)} Ù†ØªÙŠØ¬Ø©', 'success')
        except Exception as e:
            flash(f'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¨Ø­Ø«: {str(e)}', 'danger')
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø§Ù„Ù…ØªØ§Ø­Ø© Ù„Ù„Ø¨Ø­Ø«
    available_tables = [
        {'value': 'orders', 'label': 'Ø§Ù„Ø·Ù„Ø¨ÙŠØ§Øª'},
        {'value': 'purchase_invoice', 'label': 'ÙÙˆØ§ØªÙŠØ± Ø§Ù„Ø´Ø±Ø§Ø¡'},
        {'value': 'technician_due', 'label': 'Ù…Ø³ØªØ­Ù‚Ø§Øª Ø§Ù„ÙÙ†ÙŠÙŠÙ†'},
        {'value': 'audit_log', 'label': 'Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚'},
        {'value': 'material_consumption', 'label': 'Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…ÙˆØ§Ø¯'}
    ]
    
    return render_template('archive/search.html', 
                         results=results,
                         search_params=search_params,
                         available_tables=available_tables)

@app.route('/archive/restore/<table_name>/<int:record_id>', methods=['POST'])
@login_required
@require_archive_permission('manager')
def restore_archived_record_route(table_name, record_id):
    """Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø³Ø¬Ù„ Ù…Ù† Ø§Ù„Ø£Ø±Ø´ÙŠÙ"""
    
    restore_reason = request.form.get('restore_reason', 'Ø·Ù„Ø¨ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…')
    
    try:
        success = restore_archived_record(table_name, record_id, restore_reason)
        
        if success:
            flash(f'ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„ Ø±Ù‚Ù… {record_id} Ù…Ù† {table_name} Ø¨Ù†Ø¬Ø§Ø­', 'success')
            
            # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚
            add_audit_log(
                action_type='Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ù…Ù† Ø§Ù„Ø£Ø±Ø´ÙŠÙ',
                entity_type=table_name,
                entity_id=record_id,
                details=f'ØªÙ… Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„ Ù…Ù† Ø§Ù„Ø£Ø±Ø´ÙŠÙ. Ø§Ù„Ø³Ø¨Ø¨: {restore_reason}',
                user_id=current_user.id
            )
        else:
            flash(f'ÙØ´Ù„ ÙÙŠ Ø§Ø³ØªØ¹Ø§Ø¯Ø© Ø§Ù„Ø³Ø¬Ù„ Ø±Ù‚Ù… {record_id}', 'danger')
    
    except Exception as e:
        flash(f'Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©: {str(e)}', 'danger')
    
    return redirect(request.referrer or url_for('archive_dashboard'))

@app.route('/archive/manual/<table_name>/<int:record_id>', methods=['POST'])
@login_required
@require_archive_permission('manager')
def manual_archive_record(table_name, record_id):
    """Ø£Ø±Ø´ÙØ© Ø³Ø¬Ù„ ÙŠØ¯ÙˆÙŠØ§Ù‹"""
    
    archive_reason = request.form.get('archive_reason', 'Ø£Ø±Ø´ÙØ© ÙŠØ¯ÙˆÙŠØ©')
    
    try:
        success = archive_single_record(table_name, record_id, archive_reason, 'manual')
        
        if success:
            flash(f'ØªÙ… Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„ Ø±Ù‚Ù… {record_id} Ù…Ù† {table_name} Ø¨Ù†Ø¬Ø§Ø­', 'success')
            
            # ØªØ³Ø¬ÙŠÙ„ ÙÙŠ Ø³Ø¬Ù„ Ø§Ù„ØªØ¯Ù‚ÙŠÙ‚
            add_audit_log(
                action_type='Ø£Ø±Ø´ÙØ© ÙŠØ¯ÙˆÙŠØ©',
                entity_type=table_name,
                entity_id=record_id,
                details=f'ØªÙ… Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„ ÙŠØ¯ÙˆÙŠØ§Ù‹. Ø§Ù„Ø³Ø¨Ø¨: {archive_reason}',
                user_id=current_user.id
            )
        else:
            flash(f'ÙØ´Ù„ ÙÙŠ Ø£Ø±Ø´ÙØ© Ø§Ù„Ø³Ø¬Ù„ Ø±Ù‚Ù… {record_id}', 'danger')
    
    except Exception as e:
        flash(f'Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø£Ø±Ø´ÙØ©: {str(e)}', 'danger')
    
    return redirect(request.referrer or url_for('dashboard'))

@app.route('/archive/bulk-archive', methods=['GET', 'POST'])
@login_required
@require_archive_permission('manager')
def bulk_archive():
    """Ø£Ø±Ø´ÙØ© Ø¬Ù…Ø§Ø¹ÙŠØ© Ù„Ù„Ø³Ø¬Ù„Ø§Øª"""
    
    if request.method == 'POST':
        table_name = request.form.get('table_name')
        record_ids = request.form.getlist('record_ids')
        archive_reason = request.form.get('archive_reason', 'Ø£Ø±Ø´ÙØ© Ø¬Ù…Ø§Ø¹ÙŠØ©')
        
        if not record_ids:
            flash('Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ø£ÙŠ Ø³Ø¬Ù„Ø§Øª Ù„Ù„Ø£Ø±Ø´ÙØ©', 'warning')
            return redirect(request.referrer)
        
        # ØªØ­ÙˆÙŠÙ„ Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø¥Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù…
        try:
            record_ids = [int(rid) for rid in record_ids]
        except ValueError:
            flash('Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø³Ø¬Ù„Ø§Øª ØºÙŠØ± ØµØ­ÙŠØ­Ø©', 'danger')
            return redirect(request.referrer)
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„Ø¬Ù…Ø§Ø¹ÙŠØ©
        result = archive_batch_records(table_name, record_ids, archive_reason)
        
        if result['success']:
            flash(f'ØªÙ… Ø£Ø±Ø´ÙØ© {result["successful_count"]} Ø³Ø¬Ù„ Ø¨Ù†Ø¬Ø§Ø­ Ù…Ù† Ø£ØµÙ„ {result["total_records"]}', 'success')
            
            if result['failed_count'] > 0:
                flash(f'ÙØ´Ù„ ÙÙŠ Ø£Ø±Ø´ÙØ© {result["failed_count"]} Ø³Ø¬Ù„', 'warning')
        else:
            flash(f'ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©: {result.get("error", "Ø®Ø·Ø£ ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ")}', 'danger')
        
        return redirect(request.referrer or url_for('archive_dashboard'))
    
    return render_template('archive/bulk_archive.html')

@app.route('/archive/auto-archive', methods=['POST'])
@login_required
@require_archive_permission('admin')
def trigger_auto_archive():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ© ÙŠØ¯ÙˆÙŠØ§Ù‹"""
    
    try:
        results = auto_archive_eligible_records()
        
        total_archived = 0
        messages = []
        
        for table, result in results.items():
            if isinstance(result, dict) and result.get('success'):
                count = result['successful_count']
                total_archived += count
                messages.append(f'{table}: {count} Ø³Ø¬Ù„')
        
        if total_archived > 0:
            flash(f'ØªÙ… Ø£Ø±Ø´ÙØ© {total_archived} Ø³Ø¬Ù„: {", ".join(messages)}', 'success')
        else:
            flash('Ù„Ø§ ØªÙˆØ¬Ø¯ Ø³Ø¬Ù„Ø§Øª Ù…Ø¤Ù‡Ù„Ø© Ù„Ù„Ø£Ø±Ø´ÙØ© Ø­Ø§Ù„ÙŠØ§Ù‹', 'info')
    
    except Exception as e:
        flash(f'Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØ©: {str(e)}', 'danger')
    
    return redirect(url_for('archive_dashboard'))

@app.route('/archive/statistics')
@login_required
@require_archive_permission('viewer')
def archive_statistics():
    """Ø¹Ø±Ø¶ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…ÙØµÙ„Ø© Ù„Ù„Ø£Ø±Ø´ÙŠÙ"""
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø´Ø§Ù…Ù„Ø©
    stats = {}
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„
    table_stats = ArchiveStatistics.query.all()
    stats['tables'] = table_stats
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª (Ø¢Ø®Ø± 30 ÙŠÙˆÙ…)
    thirty_days_ago = datetime.now(timezone.utc) - timedelta(days=30)
    
    operation_stats = db.session.execute("""
        SELECT 
            operation_type,
            status,
            COUNT(*) as count,
            AVG(duration_seconds) as avg_duration,
            SUM(record_count) as total_records
        FROM archive_operations_log 
        WHERE operation_start >= %s
        GROUP BY operation_type, status
    """, [thirty_days_ago]).fetchall()
    
    stats['operations'] = operation_stats
    
    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… (Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„Ø§Ø³ØªØ¹Ø§Ø¯Ø©)
    usage_stats = db.session.execute("""
        SELECT 
            DATE(operation_start) as date,
            operation_type,
            COUNT(*) as count
        FROM archive_operations_log 
        WHERE operation_start >= %s
        AND operation_type IN ('search', 'restore')
        GROUP BY DATE(operation_start), operation_type
        ORDER BY date DESC
    """, [thirty_days_ago]).fetchall()
    
    stats['usage'] = usage_stats
    
    # Ø­Ø¬Ù… Ø§Ù„Ø£Ø±Ø´ÙŠÙ Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    main_db_size = db.session.execute("""
        SELECT 
            table_name,
            ROUND(((data_length + index_length) / 1024 / 1024), 2) AS size_mb,
            table_rows
        FROM information_schema.tables 
        WHERE table_schema = DATABASE()
        AND table_name IN ('orders', 'purchase_invoice', 'technician_due', 'audit_log')
    """).fetchall()
    
    stats['size_comparison'] = main_db_size
    
    return render_template('archive/statistics.html', stats=stats)
```

---

Ù‡Ø°Ø§ ÙƒØ§Ù† **Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø«Ø§Ù†ÙŠ** Ù…Ù† Ø®Ø·Ø© Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ø±Ø´ÙØ© Ø§Ù„Ø´Ø§Ù…Ù„ØŒ ÙˆØ§Ù„Ø°ÙŠ Ø±ÙƒØ² Ø¹Ù„Ù‰ **Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ‚Ù†ÙŠ**. 

**ğŸ“Œ Ù…Ù„Ø§Ø­Ø¸Ø© Ù…Ù‡Ù…Ø©:** Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙØ¹Ù„ÙŠ ÙˆØ§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªÙ‚Ù†ÙŠ. Ù„Ø§ ØªØ¨Ø¯Ø£ Ø§Ù„ØªÙ†ÙÙŠØ° Ø­ØªÙ‰ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ§Ø¹ØªÙ…Ø§Ø¯ Ø¬Ù…ÙŠØ¹ Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ø®Ø·Ø©.
